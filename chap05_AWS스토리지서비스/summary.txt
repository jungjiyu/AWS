
5.1 스토리지 개요

스토리지 storage 
	: 데이터 보관 장소. 저장 장치.
	: USB, 외장하드( SSD, HDD ) 등
		: 외장 하드디스크- USB 처럼 컴퓨터 외부에 장착하여 사용 가능한 추가 스토리지 공간
			: 그러니까 SSD, HDD 하면 아 컴터 외부 탈부착 가능한 추가 스토리지 공간~ 이럼 된다.

--------------------------------------------------------------------------------------------------------------

5.2 스토리지 서비스 및 주요 기능


저장소 프로토콜 >> 스토리지 자원을 공유하기 위한 프로토콜
	SCSI Small Computer System Interface (스카시)
		: 물리적으로(=네트워크 통한것이아닌 물리적 케이블 같은걸로), 병렬로(=여러 데이터 비트를 동시에 전송) 데이터를 전송.공유
			: 인터넷을 사용하는 방식이 아니기 떄문에 원거리는 지원 안함
		: SCSI 가 진화한건 (iSCSI 가 아니라) SAS 
			: iSCSI 는 진화했다기 보다는, 인터넷에서도 가능하게 "확장" 한 것.

	iSCSI Internet Small Computer System Interface (아이스카시)
		: 네트워크 통한 데이터 전송, 블록 수준에서 작동 . SCSI 명령을 네트워크를 통해 전송할 수 있도록 한 것.
		: 직접적인 계층 관리 불가
		: 앞에 i 소문자 맞음

	FC Fiber Channel (에프시. 파이퍼 채널)
		: 광케이블을 사용한 "빠른" 데이터 전송, 블록 수준에서 작동
		: 좀 빠른 iSCSI 라고 보면 됨.

	NFS Network File System 
		: 네트워크를 통한 데이터 전송, 파일 수준에서 작동.공유
			: 파일별 조작을 통해 계층 관리 가능

		: iSCSI vs  NFS >> 둘다 원격 자원을 로컬 자원처럼 사용할 수 있게 해주는 기술
			iSCSI : 블록 장치 수준의 접근. 그러니까 (로컬디스크같은) 장치 자체를 공유하는 느낌
			NFS : 파일 시스템 수준의 접근. 그러니까 다같이 파일/폴더만 공유하는 느낌



	: https://blog.naver.com/zagatoson/222446702817



볼륨 volume >> 데이터 저장을 위한 논리적 단위. 장치
	: 그러니까 볼륨 --> (데이터 저장 용도의) 단위 || 장치 라고 해석하면 된다.
		: ex ) 블록스토리지 볼륨 --> (데이터 저장 용도의) 블록스토리지 장치 .



SAN 과 NAS >> 서버와 스토리지의 연결 방식. 여러 매체에서 네트워크를 통해 접속해 파일을 공유 가능하게 함.
	: NAS Network Attached Storage >> 네트워크로 통신하여 저장장치에 연결
		: TCP/IP 와 같은 표준 네트워크를 이용하는 스토리지를 총칭
		: 덜 비싸고 성능이 딸림

	: SAN Storage Area Network >> 스토리지 전용 네트워킹
		: 대규모 네트워크 사용자를 위해 저장장치를 데이터 서버와 연결하여 별도의 네트워크로 관리하는 고속 네트워크 시스템
		: 광케이블을 사용한 네트워크를 이용하는 스토리지를 이용하는 스토리지를 총칭 
			: 단순하게 생각하면 광을 이용한 네트워크 == SAN 표준
			: 광케이블을 사용하는 만큼 원거리에 분산된 저장장치에서 데이터를 빠른 속도로 주고받을 수 있음
			: 광케이블 전용 switch 와 연결하여 사용하게됨
		: 비싸고 성능 좋음
	: https://www.dknyou.com/blog/?q=YToxOntzOjEyOiJrZXl3b3JkX3R5cGUiO3M6MzoiYWxsIjt9&bmode=view&idx=6123731&t=board


	: SAN Storage Area Network >> 서로 다른 종류의 데이터 저장 장치를 한 데이터 서버에 연결하여 총괄적으로 관리해주는 네트워크
	: NAS Network Attached Storage >> 네트워크에 연결된 파일 수준의 컴퓨터 기억 장치
		: 서로 다른 네트워크 클라이언트에 데이터 접근 권한 제공



		

참고 - 2장에서 EC2 인스턴스 다룰 때 언급되었던 스토리지
	1. instance store : 인스턴스에 바로 붙어있는 저장소로, 인스턴스 생성시 기본적으로 함께 생성됨
		: 일부 인스턴스 유형은 지원하지 않음 주의
		: 직접 붙어있다보니 I/O 가 빠르다
		: 인스턴스의 중지/종료와 함께 데이터가 모두 손실된다 >> 임시적인 저장소로만 활용해야된다. (장기적으로 저장할 목적으론 x)

	2. Amazon EBS Elastic Block Store : 인스턴스(의 외부)에 네트워킹을 통해 연결 및 제거를 하는 형태로 구성되는 블록 스토리지.
		: 외장 하드디스크와 비슷한 개념. 
 
		: 관리콘솔을 통해 영구 저장, 스냅샷을 생성해 백업이 가능



AWS 제공 스토리지 서비스 종류
	1. 블록 block 스토리지 : 데이터를 "블록" 이라는 개별 단위로 분할하여 저장했다가, 서버에서 파일 요청 시 블록들을 재구성하여 하나의 데이터로 서버에 전달해주는 스토리지
		: 각 블록은 고유 식별자가 부여되어 저장된다.
		: 클라우드 환경에서 블록 스토리지의 각 블록은 가상 머신 인스턴스에 위치
			: 일반 컴퓨터에 하드 디스크를 추가하여 C 드라이브, D 드라이브 처럼 논리적으로 구분해서 사용하는 것과 유사
		: 호스트에서 파일 시스템 생성
		: FC , iSCSI 를 이용하여 접근 >> 매우 빠르고, 장치 수준의 접근을 하게 된다
			: FC 로 접근하면 광케이블 쓰므로 당연히 빠른거고, iSCSI 를 사용하여 접근한다 해도 이때의 네트워크는 단순한 네트워크가 아닌 고속 네트워크를 의미해서 쨌뜬 빠르다
			: SAN 또는 가상 머신의 디스크로 사용됨
		: Amazon EBS,  (2장에서 언급된) instance store 

	2. 파일 file 스토리지 : 파일 수준, 파일 기반 스토리지
		: 디렉터리 구조로 파일을 저장
		: 계층 구조를 이룸 
			: 각 파일은 폴더에 종속되고, 폴더는 다른 폴더에 종속되는 등
		: 스토리지에서 파일 시스템 생성
		: NFS , CIFS 를 이용하여 접근 >> 그냥 저냥 속도이고, 파일 수준의 접근을 하게 된다
			: NAS 에 사용됨
		: Amazon EFS

	3. 객체 object 스토리지 : 데이터 조각을 가져와 객체로 지정, 개별 단위로 저장
		: (계층구조가 아닌) 평면적인 주소 공간
		: 메타데이터로 구성된 객체마다 ID(고유 식별자) 부여
		: OS나 파일 시스템에 의존하지 않으며 데이터 저장하고, 객체에 쉽게 접근 가능
			: 객체의 키(이름)만 알고 있으면 빠르게 대상 검색 가능
				: REST API 사용
		: 저장할 수 있는 데이터의 수와 파일 크기에 제한 없고, 데이터 저장 총 용량이 거의 무제한
		: 대량의 데이터를 저장하거나 다수의 서버에서 해당 데이터에 접근해야하는 경우 용이
		: 대부분의 서비스의 스토리지 서비스로써 사용됨
		: Amazon S3


--------------------------------------------------------------------------------------------------------------

5.3 Amazon EBS


주기억장치, 보조기억장치
	주기억장치 >> 현재 CPU가 처리하고 있는 내용을 저장하고 있는 기억 장치. 
		: 속도 빠름
		: 전원이 꺼지면 내용이 사라짐	
		: 대표적으론 ROM , RAM 
			ROM Read Only Memory
			Random Access Memory

	보조기억장치 >> 주기억 장치의 용량 부족을 보완하기 위해 사용하는 외부 장치
		: 속도는 좀 느림
		: 전원이 꺼져 있을 때도 정보 유지
		: 대표적으론 SSD, HDD 가 있다.


SSD vs HDD >> 둘다 보조기억 장치. 요즘에는 SSD 쓰지, HDD 안쓴다.
    HDD HardDisk Drive >>
	: 플래터 "디스크"를 이용하여 정보를 (물리적으로) 저장하는 장치 
	: 속도가 느리고 전기도 많이 잡아먹지만 용량당 싸다
	: 하드. 하드디스크드라이브.

    SSD Solid State Drive . Solid State Disk >>
	: "플레시 메모리"와 "반도체"를 이용하여 정보를 (전기적으로) 저장하는 장치
		: 플래시 메모리(Flash Memory) >> 전원이 끊겨도 기억된 내용이 지워지지 않으면서 입력과 수정이 쉽도록 개발한, 빠른 속도의 기억 장치
	: 용량 당 가격이 비싸지만 속도가 빠르고 저전력 
	; https://suzzeong.tistory.com/42
	; https://velog.io/@piczo/IT-%EC%A7%80%EC%8B%9D-%EB%B3%B4%EC%A1%B0%EA%B8%B0%EC%96%B5%EC%9E%A5%EC%B9%98-HDD-SSD


IOPS Input/Output opernations Per Second : 초당 처리되는 I/O 갯수. (= HDD, SDD 등 저장장치의 속도 ) 


프로비저닝 Provisioning : 인프라를 구축/할당하는 과정.
	클라우드에서의 하드웨어 프로비저닝 >> 필요한 하드웨어 자원을 할당하는 과정
	



Amazon EBS Elastic Block Store 
	: EC2 인스턴스에 사용 가능한 블록 스토리지 볼륨(장치)을 제공하는 서비스
	: 데이터를 일정한 크기의 블록으로 분산 저장
		: 볼륨 위에 파일 시스템을 생성하거나 하드디스크 드라이브 같은 블록 디바이스를 사용하는 것처럼 볼륨을 사용 가능

	: "운영체제"에 외장하드디스크를 연결해서 데이터를 저장하는 꼴
		: 그러니까, EC2의 운영체제위에서, 일종의 애플리케이션 수행하는 것처럼, 저장공간이 동작된다는 것.
			: EC2는 컨테이너가 아닌, 가상머신을 제공하는 서비스로, 가상머신 답게 운영체제가 있다.

	: 인스턴스와 EBS 볼륨의 관계 >> 독립적이며, 고속 네트워크로 연결되어있음. 
		: 인스턴스와 EBS볼륨은 데이터 수명 시간이 독립적임 --> 서로 연결된 인스턴스와 볼륨을 사용하다, 해당 인스턴스를 삭제해도 볼륨은 계속 사용할 수 있고, 그 볼륨에 저장된 데이터도 다른 인스턴스와 연결하여 이어서 사용할 수있다.
		: 인스턴스와 EBS볼륨이 (독립적이지만) 고속 네트워크로 연결되있기에 , 로컬에 있는 것처럼 쓸 수 있다.
			: 하나의 인스턴스에 여려개의 ESB 볼륨을 연결하여 사용할 수 있고, 한 볼륨은 한번에 하나의 인스턴스에만 연결되있을 수 있다.

	: EBS 와 가용영역 >> EBS 는 가용영역 단위의 서비스. 즉, 특정 가용영역에 종속적
		: 해당 AZ 내의 E2C 인스턴스에만 연결 가능 
		: (데이터 손실 방지를 위해) 해당 가용 영역 내에서 자동으로 데이터를 복제해둔다
		: 특정 AZ 내에서 고성능 스토리지가 필요한 경우에 사용함

	: Amazon EBS 암호화 encryption 기능으로, 암호화된 EBS 볼륨을 생성 가능
		: AES-256 사용
	: 서비스를 중단할 필요 없이 EBS 볼륨 수정( 유형, 크기, IOPS 용량 등) 가능
		

	: EBS 볼륨 유형
	     : 뭐 처음 생성 이후에도 서비스 중단 없이 수정 가능하니까 넘 심각하게 생각하지 x
		1. SSD 유형 << OS 영역이나, 일반 db 보관용 스토리지 유형 등으로 사용
			(1) 범용 SSD 그룹 (gp2 및 gp3) : 일반적인 경우 사용
			(2) 프로비저닝된 IOPS SSD 볼륨( io1 및 io2 ) :  ㅈㄴ 빠른 접근 필요한 경우 사용

		2. HDD 유형 << 속도와 상관 없이 용량 많이 필요한 경우 추천 ( ex : 데이터 분석 )
			(1) 처리량 최적화 HDD 볼륨 (st1) : 자주 접근하는 경우 사용
			(2) 콜드 HDD 볼륨 ( sc1 ) : 자주 접근하지 않는 경우 사용


	: EBS 볼륨 snapshot >> 특정 시점의 상태를 (사진처럼) 그대로 찍는 기능.  
		: 백업파일 만드는 느낌. 해당 시점의 상태로 돌아갈 수 있다.
		: 이 스냅샷 파일들은 보통 S3 스토리지에 저장된다.
		: 증분식 백업방식을 사용한다. 
			: 증분식 백업 Incremental Backup >> ( 전체를 다 저장하는게 아니라 ) 변화된 부분만 저장한다.
				: 젤 첨 백업에서는 일단 전체 데이터를 백업하고, 이후 백업부터는( 2번째 백업 부터는 ) 새로 변경된 데이터에 대해서만 백업한다.
				: 이름이 "증분" 인건  "increment" , 그러니까 증가한 분량에 대해서만 저장하는 거라서 그렇다.
			: https://inpa.tistory.com/entry/AWS-%F0%9F%93%9A-AMI-Snapshot-%EA%B0%9C%EB%85%90-%EB%B0%B1%EC%97%85-%EC%82%AC%EC%9A%A9%EB%B2%95-%F0%9F%92%AF-%EC%A0%95%EB%A6%AC

			: 증분식 백업 방식을 사용하여 , 그러니까 스냅샷을 찍을 당시엔 겹치지 않는 내용만 찍은 거지만 , 그렇다고 해서 백업할 때 모든 앞선 스냅샷들을 일일이 종합해서 백업시키는게 아니라 해당 스냅샷 한 장 가지고서만 백업을 진행시킨다. 
				: 이게 가능한 이유는 스냅샷을 S3 스토리지에 저장할 때, S3 스토리지의 백엔드에서 앞선 내용을 반영/참조하여, 통합하여 저장시켜줘 해당 스냅샷 자체로써 복원 가능하게 된다고 한다.

		: 스냅샷은 다른 계정으로 공유할 수도 있고, 다른 리전으로 (수동으로) 복제도 가능

	: how to use 
		1. AWS 관리 콘솔에서 커스터마이징해서(필요한 용량과 성능에 맞춰서) 볼륨 생성	
		2. EC2 인스턴스에 연결
		3. 파일 시스템 포맷 
			: 파일 시스템 포맷은 운영체제 마다 다르다 (리눅스 - xfs || ext4 , 윈도우 - NTFS )
		4. 해당 볼륨을 서버에서 마운트한 후 데이터를 해당 디렉터리에 저장하여 사용

	: 인스턴스에 연결된 EBS 볼륨의 구성을 동적으로 변경도 가능
	: 데이터베이스와 같이 데이터 출입이 빈번한 서비스에 적합
	: https://jibinary.tistory.com/150




Amazon EFS Elastic File System 
	: "리눅스" 인스턴스를 위한, "완전 관리형" 네트워크 파일 시스템
		: 리눅스 인스턴스 전용이라고, 윈도우 인스턴스 에는 사용 불가하고, 우분투도 연결이 가능하긴 한데 설정이 약간 복잡하다.
		: 완전 관리형 >>  사용자가 직접 관리할 필요없이, AWS 클라우드 서비스가 자동으로 모든 관리 작업을 처리해준다.
		: 구체적으론 다음을 관리해준다 
			1. 하드웨어 프로비저닝: 서비스가 필요로 하는 "전반적인" 하드웨어 자원을 할당. 마련.
				: 하드웨어 프로비저닝 vs 오토스케일링 
					하드웨어프로비저닝 >> 서비스의 초기 하드웨어 환경을 구축하는 과정
					오토스케일링 >> 하드웨어프로비저닝을 통해 구성된 환경을 동적으로 수정하는 과정
						: (아래에 적었지만) 오토스케일링은 유지관리에 해당
						

			2. 유지 관리: 하드웨어, 소프트웨어의 유지 관리 작업(예: 패치, 업그레이드 등)
				: 오토스케일링 기능이 여기에 해당

			3. 소프트웨어 구성: 파일 시스템의 구성, 설정, 최적화 등
			4. 모니터링: 자동 모니터링 해준다

	: EFS 와 인스턴스 >> 하나의 EFS에 여러 인스턴스가 동시에 마운트 및 사용 가능

	: EFS 와 가용영역 >> EFS 는 여러 가용영역에 걸쳐 사용 가능
		: EFS 는 동일한 리전 내의 모든 AZ 에서, 동시에 접근이 가능
		: 분산 파일 시스템이 필요한 경우 유용

	: NFS 표준 프로토콜 기반 연결 지원
		: 온프레미스 환경의 NAS 와 유사하게 활용 가능
			: ex) 사용자 홈 디렉터릴 공유하여 애플리케이션을 개발 가능
	: IOPS 가 높고 용량이 커서 처리량이 많고 대기시간 짧음
	: 오토스케일링 기능 있어 용량 부족 걱정 필요 없음
	
	: https://inpa.tistory.com/entry/AWS-%F0%9F%93%9A-EFS-%EA%B0%9C%EB%85%90-%EC%9B%90%EB%A6%AC-%EC%82%AC%EC%9A%A9-%EC%84%B8%ED%8C%85-%F0%9F%92%AF-%EC%A0%95%EB%A6%AC

--------------------------------------------------------------------------------------------------------------

5.4 Amazon S3


HTTP Hypertext Transfer Protocol :  인터넷상에서 데이터를 주고 받기 위한 서버/클라이언트 모델을 따르는 프로토콜
	: Connectless >> resposne을 받으면 연결을 끊어버린다 
	: Stateless >> 상태 유지 안한다


REST Representational State Transfer >> 자원을 이름으로 구분하여 해당 자원의 상태를 주고받는 모든 것. 그러니까 HTTP URI로 리소스를 명시하고, HTTP Method를 통해 해당 리소스에 대한 CRUD 을 적용하는 것
	REST API >> REST를 기반으로 만들어진 API
	RESTful >> REST 의 원리를 따르는 시스템.


고가용성 >> 거의 언제든지 사용 가능
내구성 >> 완전 튼튼.


Amazon S3 Simple Storage Service :  데이터를 버킷 내 객체로 저장하는 객체 스토리지 서비스.
	: S3 구성요소 
		: 객체 object >> S3 에 저장되는 데이터
			: 그러니까 일반적인 파일 시스템의 파일과 같은 기본 단위가 , S3 서비스에선 "객체"
			: 객체 구성 요소
			    : key 와 version ID 는 오브젝트 간 고유식별자가 된다. (딱 object ID 가 따로 있는게 아님). 그리고 이 식별자를 기반으로 접근하게 된다.
				: key >> 오브젝트의 이름
					: 일반적인 파일 시스템의 절대경로 개념.
					: 폴더 hierachy를 prefix 로 포함한다.

				: Version ID >> 버전 ID
					: 오브젝트를 버킷에 추가할 때 자동으로 생성되는 고유한 String 값이다.
					: 말 그대로, 동일한 오브젝트에 대해 버전을 구분하는데 쓰인다
					: 버전별 관리가 가능하다.

				: Value >> 실제 컨텐츠
				
				: Metadata >> name-value 형태의 부가정보 
					: 시스템 메타데이터 >> S3가 미리 정의한 메타데이터
						: ex ) Date , Content-Length

					: 사용자 정의 메타데이터 >> 사용자가 직접 정의한 메타데이터 

		: 버킷 bucket >> 객체 저장소. 데이터 스토리지를 위한 S3 의 기본 container . 
			: 그러니까 일반적인 파일 시스템의 폴더과 같은 기본 단위가 , S3 서비스에선 "버킷"
			: 객체는 반드시 버킷에 저장되어야됨
			: 생성된 후에는 버킷명과, 속한 리전을 변경 불가
			: S3 버킷 내에 있는 객체에 대해 여러 가용영역에 걸쳐 데이터를 복제해둬서 데이터 손실에 대비한다. >> 내구성. 고가용성
				: 기본적으로는 하나의 리전에 대해 3개 이상의 가용영역에 복제해둔다.
	
	: S3 스토리지 클래스 >> 그러니까 S3 스토리지 유형.
	    : 주로 액세스 빈도 고려하여 선택하는 듯.
		1. standard : 접근 빈도가 잦은 유형.
			: 가장 일반적인 유형으로, 사용한만큼 비용 지불.
			: 사용한만큼은 저장용량에 따른 비용 뿐 아니라, 데이터 검색/요청 등에 대한 비용도 말하는임. (요청이 들어오면 반환해주는것도 일이니까)
		2. intelligent-tiering : 자동으로 빈번한 접근 그룹과 간헐적 접근 그룹에 나누어 저장시킴
		3. infrequent access : 
			: infrequent access 를 줄여서 IA 로 표기하기도 한다
			(1) standard-infrequent access : 일반적인 방식으로 (최소 3개 이상의 가용영역에 데이터를 복제하여) 저장
			(2) one zone-infrequent access : 하나의 가용영역에만 데이터를 저장. 내구성이 낮다.

		4. S3 glacier : 아카이빙용 스토리지. 
			: 접근을 위해서라기보다는 백업용으로 보관하려는 경우 유용
			: 조회하는데 수분에서 수시간 걸림

		5. S3 glacier deep archive : 재사용이 거의 없는 데이터를 보관하는용
			: 젤 저렴하긴해도 조회하는데 수시간에서 수일이 걸릴수도 있다;;; 


	: Amazon S3 는 "웹 서버"를 제공한다 
		: 기본적으로 웹 접속이 가능하다 >> HTTP/HTTPS 프로토콜을 통해 클라이언트의 request 를 받고 이에 대한 response 를 할 수 있다
		: 자체적으로 정적 웹 콘텐츠를 반환할 수 있다
		:  WAS (Web Application Server) 까지 제공해주는건 아니라서, 동적 컨텐츠는 생성 못한다 

	: REST API 로 명령이 전달됨 ( http 메서드 기반 )
	: 내구성이 99.9999.. % . 그러니까 데이터 손실 ㅈㄴ 적다
	: 데이터 저장공간이 거의 무제한
	: S3 보안 >> 
		sol1. IAM 을 활용 <-- 뒷 챕터서 나오는 내용
			; IAM Identity and Access Management >> AWS 의 자격증명서비스
				: 주의 ) 인스턴스 생성할 때 설정하는 AMI Amazon Machine Image 용어랑 헷갈려하지 x
			: IAM 으로 접속 사용자, 데이터 접근을 관리 가능
		
		sol2. Amazon S3 버킷 정책 활용
 			: 버킷 내 모든 객체에 대한 권한 조정 가능
				: 일시적 권한 부여도 가능 ( presign ) 
			: S3 객체는 기본적으로 외부 사용자가 접근 불가하게 설계되어있음
				: 그러니까 루트 사용자와, 객체 소유자만 해당 객체에 접근 가능
				: 외부 사용자에 대한 접근을 허용하려면, Amazon S3 버킷 정책 가지곤 안되고, IAM을 활용해야됨.


	: https://m.blog.naver.com/techtrip/222101895713
	: https://velog.io/@ghldjfldj/AWS-S3%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80

--------------------------------------------------------------------------------------------------------------

5.5 다양한 AWS 스토리지 구성하기 

자주 까먹는거 정리
	1. 서브넷과 라우팅 테이블을 attach (연결)하는 것 == 서브넷이 해당 라우팅 테이블의 규칙을 따르게 만드는 것 != 해당 서브넷으로도 라우팅 테이블이 라우팅할 수 있게 하는 것

	2. public , private subnet >> IGW 와 직접적으로 연결되있나 여부
		: public subnet >> IGW 와 직접적으로 연결된 서브넷
		: private subnet >> IGW 와 직접적으로 연결 안된 서브넷


	3. YAML 파일의 객체 표현
		"-" >> 요소 하나의 시작
		줄바꿈 >> 나열

		ex )

		SecurityGroupIngress:
  		- IpProtocol: tcp
  		  FromPort: '80'
  		  ToPort: '80'
 		  CidrIp: 0.0.0.0/0
  		- IpProtocol: tcp
  		  FromPort: '2049'
  		  ToPort: '2049'
 		  CidrIp: 10.40.0.0/16


		을 객체로 표현하면


 		"SecurityGroupIngress": [
  		  {
   		   "IpProtocol": "tcp",
    		  "FromPort": 80,
   		   "ToPort": 80,
    		  "CidrIp": "0.0.0.0/0"
   		 },
   		 {
   		   "IpProtocol": "tcp",
     		 "FromPort": 2049,
    		  "ToPort": 2049,
    		  "CidrIp": "10.40.0.0/16"
   		 }  ]


		이다




마운트 mount 한다 >> (하드디스크같은) 물리적장치를 특정 위치(디렉터리)에 연결시켜주는 것

(리눅스) 파티션 partition >> 디스크 공간의 분할.
	: ex) /dev/hda 를 3파티션으로 /dev/hda1 ,  /dev/hda2 ,  /dev/hda3 와 같이 쪼갠다
	: 장치파일명 과 파티션명>> 각 디스크, 파티션에 대응하는 장치파일명은 통상적인 규칙에 따라 지어짐
		1. IDE 디스크 >> /dev/hd[어쩌구]
		    : [어쩌구] 
			알파벳 a-z >> 해당 디렉터리가 어떤 디스크에 대응하는지 나타냄
				: 디스크 개수에 따라 a-z 가 들어감 
				: ex) IDE 디스크 3개면 /dev/hda, /dev/hdb,  /dev/hdc 로 구성됨 

			숫자 >> 해당 디렉터리가 어떤 디스크의 파티션인지를 나타냄
				: ex)   /dev/hda 디스크를 3개의 파티션으로 나누면  /dev/hda1, /dev/hda2,  /dev/hda3 으로 나뉘는 거임


		2. SCSI 디스크 >> /dev/sd[어쩌구]
			: [어쩌구] 는 IDE 디스크와 비슷하게 a-z, 1 ...9  들어감

	: 파티션을 하는 이유 >> 각 공간끼리 서로 영향을 주지 않게 하기 위해. 주로 서로 다른 OS 를 설치하기 위해.

	: 파티션 관련 절차
		0. 디스크 추가 
		1. 파티션 생성 >> fdisk [장치파일명]
			: /dev/hdb 장치의 파티션 생성 ex ) fdisk /dev/hdb

		2. 각 파티션에 파일 시스템 생성 >> mkfs [파일시스템유형] [파티션명]
			: 그러니까 해당 파티션을 파일시스템화 하는 것
			    : 파일 시스템을 만든다 == 생성된 파티션(의 유형대로)으로 format 을 한다
				: 파일시스템 유형  >> ext2, ext3, swap , FAT, etc , iso9660 등 ..
				: /dev/hdb3파티션에 ext3 파일 시스템 만들기 ex )  mkfs.ext3 /dev/hdb3
				
		3. (파일시스템이 생성된) 각 파티션에 마운트 
			: 그러니까 장치에 접근하여 활용할 수 있도록, 접근 가능한 디렉터리에 덮어 씌우는 작업(장치파일은 직접적으로 사용불가하기 때문에 반드시 해줘야된다)
			    : 마운트 한다 == 파일시스템과 하드디스크를 연결해준다
			    : 그러니까 /dev/hda1 의 mountpoint 가 /mnt/new 라고 하는 것은, /mnt/new에 접근하는 것은 사실상 /dev/hda1 에 접근하는 것인란 의미이다.
			(1) 마운트 포인트 생성 >> mkdir [마운트포인트명]
				: 마운트포인트 == 마운트용 디렉터리
					: 주로 /mnt/파일명 와 같이 작명함
			(2) 마운트 진행 >> mount [파티션명] [마운트포인트]


		4. 자동 마운트 설정
			: 한번 마운트 되게 설정했다고, 재부팅 할 때마다 매번 마운트 되는게 아니다. 자동 마운트 설정을 따로 해주지 않으면 매번 마운트 작업(연결작업)을 수행해줘야된다. 
			: 자동 마운트 설정 한다 == /etc/fstab 에 해당 장치를 추가한다 == 재부팅할 때 자동으로 마운트되게 한다

	: 관련 명령어
		1. fdisk -l >> 장치파일, 파티션 확인 
		2. df -hT >> ( 파티션으로 나누어진)  파일 시스템 디스크 공간 확인 
			-h >> 사용자가 편리하게 볼 수 있게 용량 표시
			-T >> 종류 표시
		3. cat /etc/fstab >> 파티션 정보 및 자동 마운트 정보 출력
			/etc/fstab >>모든 파티션, 자동 마운트 대한 정보가 있음
		
		4. lsblk -f >> LiStBLocK.  시스템에 연결된 블록장치(디스크 및 파티션) 정보 확인
		5. blkid >> BLocKID. 블록 장치의 정보 (UUID 등) 확인
			UUID Universally Unique Identifier>> 파일시스템의 고유식별자로, 시스템 전체에서 유일하며 파티션과 다른 파티션ㅇ르 구분하는데 사용된다. 
				: 파일시스템 식별에 자주 사용된다.

	: https://rhrhth23.tistory.com/136



루트 (디바이스) 볼륨 >> 인스턴스 부팅에 사용되는 이미지가 저장되는 볼륨. 
	: 그러니까 해당 EC2 인스턴스의 OS 가 설치되는 기본 디스크로 사용되는 볼륨
	: https://galid1.tistory.com/412

데이터 (디바이스) 볼륨 >> 루트 디바이스 볼륨이 아닌 볼륨.
	: 그러니까 이미지 저장용 이외의 용도로, 추가적으로, 사용되는 볼륨



EBS , EFS, S3 와 EC2 인스턴스 
	: EBS -> 특정 EC2 인스턴스와 밀접한 관계를 가지는 스토리지. EBS 는 EC2 서비스의 하위 메뉴로 재공됨
		: EC2 인스턴스를 생성하면 내부적으로 알아서 "루트디바이스볼륨" 으로써 EBS 스토리지가 생성된다
			: 이때 생성되는 EBS 볼륨의 크기/유형은 해당 인스턴스 생성을 위해 선택한 AMI(Amazon Machine Image ) 따라 알아서 결정된다 ( 원한다면 인스턴스 생성할 때 조정 가능) 


		: EBS 볼륨의 종류
			(1) EC2 인스턴스를 생성함에 따라 자동생성되는 EBS 볼륨 >>  루트 볼륨 Root Volume
			(2) EC2 인스턴스 생성 이후에 추가 생성하는 EBS 볼륨>> 데이터 볼륨 Data Volume


		: EC2 인스턴스에 할당되는 볼륨의 장치파일명
			: 요즘엔 AWS에서 사용되는 Xen 하이퍼바이저와 Nitro 하이퍼바이저의 영향으로 linux 이건 window 이건 루트볼륨으론 xvd[어쩌구]를 더 쓰는 추세 (데이터볼륨으로썬 여전히 sd[어쩌구]를 더 선호). 그리고 ( Xen 하이퍼바이저와 Nitro 하이퍼바이저의 영향으로 ) 사용자가 sd[어쩌구]로 만들어도, 인스턴스 내에서는 xvd[어쩌구]로 나타날 수도 있음.
			
	
				1. /dev/sd[어쩌구] >> SCSI 디스크 . 부팅 장치에 대해 설정됨.
					: linux 의 EBS 및 인스턴스 스토어에 주로 권장됬었음
					: /dev/sda 등이 있다
	
				2.  /dev/xvd[어쩌구]  >>  XEN 가상 장치. 확장 장치에 대해 설정됨.
					: windows의 EBS 및 인스턴스 스토어에만 주로 권장 됬었는데 요즘엔 걍 얘가 많이 쓰임
					: /dev/xvda 등이 있다

			: https://skstp35.tistory.com/469


		: 기본적으로는 EC2 인스턴스를 종료하면 자동생성된 EBS 볼륨도 삭제되게 되어있는데, 원한다면 종료해도 유지하게 설정 가능

	: EFS -> 다수의 EC2 인스턴스와 연결되는 스토리지.  EC2 와 독립적인 별도의 스토리지 서비스로 제공됨.
	: S3 -> EC2 와 독립적인 별도의 스토리지 서비스






실습 
    : 실습 단계
	1. 실습에 필요한 기본 인프라 배포 (= 겸사겸사 EBS Root volume 생성)
		: AWS CloudFormation 을 사용하여 자동으로 배포
		: 절차 
			(1) 스택 생성 
				1. cloudformation 서비스 클릭 - 메인 화면에서 "스택 생성" 버튼 클릭
				2. 다음의 내용으로 스택 생성
					(1) 템플릿 지정 >> 
						1. 사전조건-템플릿 준비 >> 그대로(기본 템플릿 선택)
						2. Amazon S3 URL 에는 교재에서 제공한 url(https://cloudneta-aws-book.s3.ap-northeast-2.amazonaws.com/chapter5/storagelab.yaml
) 입력
							: 해당 URL 은 템플릿(YAML파일)을 다운받을 수 있는 URL임
							: 다운로드 받게되는 YAML 파일의 구성
								: 내부적으로 자주 사용되는 함수 >>
									1. !Ref >> 
									    : "!Ref 리소스명||파라미터명" 의 꼴로 활용됨
										(1) 해당 리소스의 고유 식별자(ID)를 가져옴
										(2) Parameters 섹션에 정의된 파라미터 값을 참조하는 데 사용
									2. !GetAtt >> 해당 리소스의 properties 에 정의된 속성값을 참조하는데 사용
										:  "!GetAtt 리소스논리이름.특정속성명" 의 꼴로 활용됨

								: 참고 >>
									1. IGW 는 생성한다고 걍 원하는 VPC 에 attach 되는게 아니라, "IGW 생성"에 대한 것 뿐 아니라 "attach 대한 것" 또한 따로 정의해줘야된다. 

									2. 라우팅 테이블 생성할 때 그 라우팅 테이블의 규칙까지 한번에 정의하는 꼴이 아니라, "라우팅 테이블 생성"에 대한 정의와 "특정라우팅테이블의라우팅규칙"을 따로 정의하는 꼴이다

									3. 라우팅테이블과 특정 서브넷을 연결하는 것도 별도로 정의해야된다
									

								: Parameters 섹션>> 해당 YAML 파일 내부에서 쓰이는 변수를 정의하는 부분으로, 사용자에게 입력을 받은 값으로 결정된다.
									: 이 섹션에는 다음 항목을 명시
										1. KeyName >> EC2 KeyPair의 이름
										2. LatestAmiId >> AMI의 ID
 
								: Resources 섹션>> 생성할 자원을 정의하는 부분
									: (Resources 섹션) 바로아래에는생성할리소스의논리적이름을명시
										: 해당 이름은 템플릿 내에서 고유하고, 참조 가능
										: 이 이름으로 실제 리소스가 만들어진다는게 아니라 템플릿 내에서 해당 이름으로 참조가능하게 된다는 개념인거임 (생성되는 리소스의 이름정하는건 properties 항목의 Tag 속성에서 정의 필요)
										: 이 항목 아래에는 다음 항목을 명시
											1. Type >> 리소스의 종류(VPC, subnet, EC2인스턴스, IAMRole 등 .. )를 정의
											2. Properties >> 리소스의 세부적인 속성 정의 
											    : 정의 가능한 속성
												(1) CidrBlock >> IP주소 범위 설정

												(2) AvailabilityZone >> 가용영역설정

												(3) Tags >> 태그 설정
													: key 와 value 의 형태로 구성됨
													: tip >> "- key : Name Value: 어떤이름" 으로 생성되는 리소스의 이름이 정해지게된다



								: Outputs 섹션>> 출력으로 표시할 내용 정의
									: (Outputs 섹션) 바로아래에는 정의할 출력의 논리적 이름 명시
										: 해당 이름은 마찬가지로 템플릿 내에서 고유해야하고, 참조 가능
										: 이 항목 아래에는 다음 항목을 명시 가능
											1. Value >> 출력할 값 정의
											2. Description >> 출력에 대한 설명 정의


					
					(2) 스택 세부 정보 지정 >>
						1. 스택 이름 >> storagelab
						2. 파리미터 의 keyname >> 이전에 생성한 키페어파일인 ongja_key 선택

					(3) 스택 옵션 구성 >> 그대로 냅두고 다음 누름
					(4) 검토 밑 작성 >> 맨 마지막에 있는 "사용자 지정 이름으로 IAM 리소스를 생성할 수 있음을 승인합니다" 에 체크하고 "전송" 누름
						: 5분 정도 후 인프라 배포 완료 됨
						: 최종적으로는 (YAML파일에 정의됬던대로) 다음과 같이 인프라가 구성됨
							1. VPC (CH5-VPC)
								: IP 범위 >> 10.40.0.0/16 

							2. IGW (CH5-IGW)

							3. public routing table (CH5-Public-RT) >> CH-IGW와 연결됨
								: VPC 생성시 기본 생성되는 라우팅 테이블이 아니라 별도로 정의/생성한 라우팅 테이블이다

								: YAML 파일 
									라우팅테이블정의 >> Type: AWS::EC2::RouteTable
										VpcId >> 해당 라우팅테이블이 속할 VPC 를 정의하게 된다 
											: 라우팅테이블 생성하면 (AWS에 의해) 해당 라우팅이속한 VPC에 대한 라우팅규칙(destion이 VPC 이고 target이 local인 라우팅규칙)은 자동 추가된다 >> 이는 VPC 내부의 모든 리소스가 서로 통신이 가능하게 보장한다

									라우팅테이블의 규칙정의 >> Type: AWS::EC2::Route
										RouteTableId >> 해당 라우팅테이블규칙이 속할 라우팅테이블을 정함
										DestinationCidrBlock >> Destination IP 범위 설정
										GatewayId >> Target IP 범위 설정


									라우팅테이블의 서브넷과의 attach 정의 >> Type: AWS::EC2::SubnetRouteTableAssociation
										: 라우팅 테이블은 생성한다고 자동적으로 해당 VPC 내의 모든 서브넷과 attach 되는게 


							4. subnet
								(1) CH5-Public-Subnet1
									: IP 범위 >> 10.40.1.0/24
									: attach 된 라우팅 테이블 >> CH5-Public-RT

								(2) CH5-Public-Subent2
									: IP 범위 >> 10.40.2.0/24
									: attach 된 라우팅 테이블 >> CH5-Public-RT

							5. IAM role (STGLabInstaceRole) >> S3 FULL 정책 연동

							6. InstaceProfile (STGLabRoleForInstaces) >> STGLabInstanceRole 연동

							7. EC2 instance << 이름이 STG1, STG2 이긴 하지만 앞에 EC2 가 붙어있는걸 보면 알 수 있다 싶이 얘네들 자체는 인스턴스이고, 그 내부에 EBS 스토리지가 구성되어있는 것
								(1) EC2-STG1
									: subnet >> CH5-Public-Subnet1
									: private IP >> 10.40.1.10

								(2) EC2-STG2
									: subnet >> CH5-Public-Subnet2
									: private IP >> 10.40.2.10

							8. Security Group ( CH5-SG )
								: 정의된 인바운드 규칙 
									(1) 전 세계에서 오는 HTTP(포트 80) 트래픽 허용
									(2) VPC 내부에서 오는 NFS(포트 2049) 트래픽 허용
									(3) 전 세계에서 오는 SSH(포트 22) 트래픽 허용
									(4) 전 세계에서 오는 모든 ICMP 트래픽 (Ping 포함) 허용
								    : YAML 파일
									SecurityGroupIngress >> 인바운드 규칙 정의 
										: 각 규칙은 객체의 형태로 정의된다
										IpProtocol >> 허용할 프로토콜
											: tcp, icmp 같은 거 적을 수 있다

										FromPort , ToPort >> 허용할포트의 범위 설정
											FromPort >> 허용할 시작 포트 번호 
											ToPort >>허용할 마지막 포트 번호
											tip 
												1. FromPort : 'n' 인데 ToPort: 'n' 이면 딱 그 n번 포트에 대해서면 설정이 정의되는 것
												2. 포트 개념이 없는 경우 '-1' 값을 준다
										CidrIp >> 접근허용할 IP 주소의 범위 설정



					(5) EBS 스토리지 제대로 생성됬나 확인해보기
						1. STG1(EC2인스턴스) 내부 접속하기 
						2. EBS 스토리지 기본정보 확인해보기
							(1) 디스크 여유 공간 확인 >> df -hT /dev/xvda1
								: df >> Disk Free . 디스크 여유 공간
								: -hT >> 
									-h >> 읽기 쉽게 (GB, MB 같은 단위써서) 표시
									-T >> Type. 파일 시스템 유형 표시
								: /dev/xvda1 >>  EC2 인스턴스의 루트 디스크 (파티션)
								: 출력됬던 내용 >> /dev/xvda1 파일시스템의 유형은 xfs 이고, 사이즌 8.0G이고, 2.2G 사용됬고, 앞으로 5.9G 더 이용 가능하고, 사용량을 따짐 27%이고. 루트디렉터리(/)에 마운트 됬다(= 이 /dev/xvda1 디바이스가 시스템의 루트 파일 시스템이다)
									Filesystem     Type  Size  Used Avail Use% Mounted on
									/dev/xvda1     xfs   8.0G  2.2G  5.9G  27% /

									
							(2) 디스크 정보 확인 >> lsblk 
								: lsblk >> LiStBLocK
								: 사용 가능한 디스크 디바이스와 마운트포인트 등의 정보 확인 가능
								: 출력됬던 내용 >> xvda 라는 디스크는 xvda1라는 마운트포인트가 루트인 파티션이 있다.

									NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
									xvda    202:0    0   8G  0 disk
									└─xvda1 202:1    0   8G  0 part /



							(3) 디스크 정보 확인 >> lsblk 
								: 출력됬던 내용 >>  /dev/xvda1 의 UUID 는 a1240874-6544-4673-bbbd-19b561fbcb84이다
									/dev/xvda1: LABEL="/" UUID="a1240874-6544-4673-bbbd-19b561fbcb84" TYPE="xfs" PARTLABEL="Linux" PARTUUID="03c56153-95ec-49ac-b591-21c1e161b79b"

							(4) 디바이스의 탑재 지점(자동마운트정보) 확인 >> cat /etc/fstab
								: 출력된 내용 >> UUID가 a1240874-6544-4673-bbbd-19b561fbcb84 이고 마운트 포인트가 루트이고 .. 뭐 그렇다는 내용
									UUID=a1240874-6544-4673-bbbd-19b561fbcb84     /           xfs    defaults,noatime  1   1



	2. EBS 스토리지를 추가로 생성하고 사용(= EBS Data volume 생성)
		: 절차 
			(1) EC2 인스턴스서비스의 EBS Elastic Block Store 탭의 "볼륨" 클릭 - "볼륨 생성" 버튼 클릭
			(2) 다음과 같이 설정 후 생성 버튼 클릭
				볼륨 유형 >> 범용 SSD(gp2)
				크기 (Gib) >> 20 
				가용영역 >> ap-northeast-2a
				태그 
					key >> Name
					value >> Data1

			(3) 생성된 EBS Data Volume 확인 >> 이름이 Data1 인 EBS 볼륨이 생성되있고, 상태는 In-Use(사용중) 이 아닌 Avaliabe (사용가능) 임.
				
			(4) 생성한 EBS 볼륨과 인스턴스 연결 >> 
				1. 생성한 EBS 볼륨(Data1)을 체크하고. 드롭다운 작업버튼 클릭하여 "볼륨 연결" 클릭
				2. 다음과 같이 설정후 연결 >> 해당 볼륨과 연결될 인스턴스(EC2-STG1)를 특정하고, 해당 인스턴스 내에 이 볼륨에 대한 장치파일(/dev/sdf)을 생성하게 하는 것
					(1) 인스턴스 : EC2-STG1
						: 해당 볼륨과 연결될 수 있는 인스턴스로 EC2-STG1 밖에 안뜨고, EC2-STG2 는 뜨지 않는 것은 EC2-STG1 만 이 볼륨과 같은 가용영역에 속해있기 때문.

					(2) 디바이스 : /dev/sdf
						: 그러니까 이 인스턴스 내부에 /dev/sdf 가 애초에 있고 그걸 사용할 거란 말이 아니라, 이 추가될 볼륨에 대해 /dev/sdf 라는 장치 파일을 새로 생성하겠다 하는 것
						: 이름을 xvd 로 안하고 sd 로 한 것은 , 이 볼륨은 데이터 볼륨 용이기 때문.


			(5) EC2-STG1에 SSH 접속해서, 연결한 EBS 볼륨을 사용하도록 설정
				1. EC2-STG1에 SSH 접속
				2. 디스크 정보 확인 >> lsblk 
					: 데이터 볼륨 연결하고 나니까 저번이랑 출력 내용이 좀 다른걸 확인 가능. 
					    : (기존 xvda 디스크와 더불어) 새로운 디스크인 xvdf 가 보임.  
						NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
						xvda    202:0    0   8G  0 disk
						└─xvda1 202:1    0   8G  0 part /
						xvdf    202:80   0  20G  0 disk

				3. 디스크 xvdf대해, xfs 포맷의 파일시스템 생성 >> mkfs -t xfs /dev/xvdf

				4. 마운트 
					(1) 마운트포인트 생성 >> mkdir /data
					(2) 마운트 >> mount /dev/xvdf /data
						: 아 꼭 마운트를 파티션 대해 할 필욘 없구나

				5. 마운트 제대로 됬나 확인 >>
				 	(1) lsblk  >>  xvdf 디스크의 마운트포인트 부분이 /data로 바뀐것을 확인 가능
						NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
						xvda    202:0    0   8G  0 disk
						└─xvda1 202:1    0   8G  0 part /
						xvdf    202:80   0  20G  0 disk /data


				 	(2)  df -hT /dev/xvdf  >>  xvdf 디스크의 마운트포인트 부분이 /data로 인것을 확인 가능

						Filesystem     Type  Size  Used Avail Use% Mounted on
						/dev/xvdf      xfs    20G  176M   20G   1% /data


				6. 연결한 볼륨 활용해보기(=마운트포인트 활용해보기) >> echo "EBS Test" > /data/memo.txt 하고 cat /data/memo.txt
					: 정상적으로 "EBS Test" 출력됨

					
				7. 장착할 볼륨 정보 확인 >> blkid
					/dev/xvda1: LABEL="/" UUID="a1240874-6544-4673-bbbd-19b561fbcb84" TYPE="xfs" PARTLABEL="Linux" PARTUUID="03c56153-95ec-49ac-b591-21c1e161b79b"
					/dev/xvdf: UUID="5034de6b-eec7-450d-92cb-16857cc8f27b" TYPE="xfs"


				8. 일단 fstab 확인 >> cat /etc/fstab
					: (루트 볼륨인 xvda1 대해서만 자동 마운트 설정되있고 ) xvdf 대해선 자동 마운트 설정이 안되있음을 알 수 있음
						UUID=a1240874-6544-4673-bbbd-19b561fbcb84     /           xfs    defaults,noatime  1   1

				9. fstab 수정 >> 다음을 입력 : echo "UUID=5034de6b-eec7-450d-92cb-16857cc8f27b     /           xfs    defaults,nofail   0   2" >> etc/fstab
					: 앞서 확인한 xvdf 의 UUID 를 새로 추가하여, 추가 생성된 볼륨이 재부팅 할 떄 자동으로 마운트 되게 한다


				8. 수정된 fstab 확인 >> cat /etc/fstab
				     : 정상적오르 xvdf 가 자동마운트되게 추가되있음을 확인 가능
					UUID=a1240874-6544-4673-bbbd-19b561fbcb84     /           xfs    defaults,noatime  1   1
					UUID=5034de6b-eec7-450d-92cb-16857cc8f27b     /           xfs    defaults,nofail   0   2







	3. EBS 스토리지의 볼륨 크기를 변경하고, 스냅샷 기능으로 백업
		(1) EBS 스토리지의 볼륨 크기 변경하기
			: 데이터 볼륨 뿐 아니라 루트 볼륨도 크기를 탄력적으로 조절 가능
			: 절차 
			    (1) AWS 콘솔에서, 볼륨의 크기 수정 >> 실제 물리적 장치의 스펙을 변경하는 것이라고 생각함 됨
				1. EC2 서비스의 EBS 볼륨탭의 볼륨 클릭,  EC2-STG1_Root_Volume 체크 , 작업 드롭박스 클릭하고 볼륨 수정 선택

				2. 다음과 같이 설정 후 "볼륨수정" 버튼 클릭
					(1) 볼륨 유형 : 범용 SSD(gp3)
					(2) 크기 Gib : 20
					(3) IOPS : 3000
					(4) 처리량 : 125

				3. 볼륨 목록 확인해보면 상태가 In-use-optimizing 으로 변경되고 수정되고 있음을 확인 가능
					: 3~10분 정도 걸림. 좀 더딘데 나중에 In-use 상태로 알아서 바뀐다.


			    (2)  인스턴스에 SSH 접속해 파티션, 파일 시스템 크기 조정 >> 물리적 디스크의 크기를 수정한 후 그 디스크의 파티션과 파일 시스템을 다시 설정하는 거라 생각함 됨
				: 주의 >> AWS 콘솔에서 볼륨 수정한다고, 인스턴스에 그에 따라 자동으로 파티션/파일 조정되는게 아니고 별도로 작업 해줘야된다. 
					: 뭐 꼭 매번 이렇게 수동으로 안하고 UserData 나 cloud init 같은 걸 활용해 이 과정을 자동화 할 수 있는데, 방법이 어찌됬든 단순 EBS 볼륨을 수정한다고 인스턴스에 자동 반영되는건 아니다.

 				: 파티션 조정 -> 파일 시스템 조정
					(0) EC2-STG1 인스턴스에 SSH 접속
					(1) 파티션 확장
						1. 파티션 변경 전, 파티션과 파일시스템 상태 확인 
							(1) 파티션 상태 확인 >> lsblk
							NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
							xvda    202:0    0  20G  0 disk
							└─xvda1 202:1    0   8G  0 part /
							xvdf    202:80   0  20G  0 disk /data

							(2) 파일시스템 상태 확인 >> df -hT /dev/xvda1
								Filesystem     Type  Size  Used Avail Use% Mounted on
								/dev/xvda1     xfs   8.0G  2.2G  5.9G  27% /

						2. 파티션 확장 >> growpart /dev/xvda 1 
							: 실행하면 CHANGED: partition=1 start=4096 old: size=16773087 end=16777183 new: size=41938911 end=41943007 가 출력된다

						3. 변경 후 상태 확인 
							(1) 파티션 상태 확인 : 정상적으로 증가함(8G -> 20G) >> lsblk
								NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
								xvda    202:0    0  20G  0 disk
								└─xvda1 202:1    0  20G  0 part /
								xvdf    202:80   0  20G  0 disk /data

							(2) 파일시스템 상태 확인 : 변한게 없음 >> df -hT /dev/xvda1
								Filesystem     Type  Size  Used Avail Use% Mounted on
								/dev/xvda1     xfs   8.0G  2.2G  5.9G  27% /


					(2) 파일시스템 확장
						1. 파일시스템 확장 >> xfs_growfs -d / 
							: 다음과 같은 내용이 출력됨
							    meta-data=/dev/xvda1             isize=512    agcount=4, agsize=524159 blks
							    =                       sectsz=512   attr=2, projid32bit=1
							    =                       crc=1        finobt=1, sparse=0, rmapbt=0
							    =                       reflink=0    bigtime=0 inobtcount=0
							    data     =                       bsize=4096   blocks=2096635, imaxpct=25
							    =                       sunit=0      swidth=0 blks
							    naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
							    log      =internal log           bsize=4096   blocks=2560, version=2
							    =                       sectsz=512   sunit=0 blks, lazy-count=1
							    realtime =none                   extsz=4096   blocks=0, rtextents=0
							    data blocks changed from 2096635 to 5242363



						2. 변경 후 상태 확인  >> df -hT /dev/xvda1
							: ( 80G-> 20G ) 로 정상 변경됨을 확인 가능
								Filesystem     Type  Size  Used Avail Use% Mounted on
								/dev/xvda1     xfs    20G  2.2G   18G  11% /

 
		(2) 스냅샷 기능 확인해보기	
			: 스냅샷 기능 == 백업 기능
			: EBS 스냅샷은 S3에 저장된다
			: 스냅샷은 "볼륨" 단위로 찍히고, 활용된다
				: AWS 콘솔의 EC2서비스-EBS볼륨-볼륨 탭에서, 특정 볼륨을 택하여 해당 볼륨에 대한 스냅샷을 생성하는 방식
					: 스냅샷을 찍는다는게 해당 인스턴스의 모든 볼륨에 대해 기록을 한다는게 아니라 해당 볼륨에 대해서만 이뤄지는 것

				: 스냅샷의 활용 << 역시 볼륨 단위로 실행된다. 
					case 1. 해당 스냅샷 시점으로 볼륨을 복원
						step0. 대상 인스턴스 중지시키기 
						step1. 현재 해당 볼륨과 인스턴스와의 연결을 해제  
						step2. 해당 스냅샷으로 새로운 볼륨 생성
						step3. 새로 생성한 볼륨을 해당 인스턴스와 연결하는데,  이때 디바이스명을 기존 볼륨이 쓰던 디바이스명으로 함
							: 해당 스냅샷이 루트 볼륨에 대한 거었음 루트 장치 파일명을 써야되는거고, 데이터 볼륨 대한 거였음 해당 데이터 볼룸의 장치파일명을 써야되는거임
			
						: https://show-me-the-money.tistory.com/entry/AWS-%EA%B8%B0%EC%A1%B4-EC2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%97%90-%EC%8A%A4%EB%83%85%EC%83%B7-%EB%B3%B5%EC%9B%90%ED%95%98%EA%B8%B0

					case 2. 해당 스냅샷을 새로운 볼륨으로 추가
						step0. 대상 인스턴스 중지시키기 
						step1. 해당 스냅샷으로 새로운 볼륨 생성
						step2. 새로 생성한 볼륨을 해당 인스턴스와 연결하는데,  이때 디바이스명을 새로운 디바이스명으로 함




			: 절차 
			     (1) 테스트용 파일 생성
				1. 10G 크기의 가상 파일 생성 >> fallocate -l 10G /home/10G.dummy
					: fallocate >> 지정된 크기의 파일을 빠르게 생성하는 명령어. 
						: 해당 파일의 크기만큼 디스크 공간을 차지하기만 하고, 실제 데이터를 기록하진 않는다.
				2. 파일시스템 상태 확인 >> df -hT /dev/xvda1
				    : 10G 크기의 가상 파일이 생성됬음을 확인 가능 ( used 가 2.2G -> 13G, avail 이 18G -> 7.9G .. ) 
					Filesystem     Type  Size  Used Avail Use% Mounted on
					/dev/xvda1     xfs    20G   13G  7.9G  61% /


			     (2) 스냅샷으로 백업. 스냅샷 찍기
				1. EC2 서비스의 EBS 볼륨 탭의 볼륨 클릭 , EC2-STG1_Root_volume 에 체크 , 작업 드롭박스 클릭 후 "스냅샷 생성" 클릭
				2. 설명("first snapshot") 간단히 입력 후 "스냅샷 생성" 버튼 누름
					: commit message 랑 비슷한 느낌
				3. EC2 서비스의 EBS 볼륨 탭의 스냅샷클릭후, 스냅샷 목록에서 진행 상황 확인
					: 최초 백업은 볼륨 전체에 대한 백업이므로 다소 시간이 걸리는 편


			     (3) 테스트용 파일 2차 생성
				1. 5G 크기의 가상 파일 생성 >> fallocate -l 5G /home/5G.dummy

				2. 파일시스템 상태 확인 >> df -hT /dev/xvda1
				    : 5G 크기의 가상 파일이 추가 생성됬음을 확인 가능
				    	Filesystem     Type  Size  Used Avail Use% Mounted on
				    	/dev/xvda1     xfs    20G   18G  2.9G  86% /

 
			     (4) 2차 스냅샷 찍기
				1. EC2 서비스의 EBS 볼륨 탭의 볼륨 클릭 , EC2-STG1_Root_volume 에 체크 , 작업 드롭박스 클릭 후 "스냅샷 생성" 클릭
				2. 설명("second snapshot") 간단히 입력 후 "스냅샷 생성" 버튼 누름
				3. EC2 서비스의 EBS 볼륨 탭의 스냅샷클릭후, 스냅샷 목록에서 진행 상황 확인
					: 증분백업 방식이므로 젤 처음 스냅샷에 비해 빠르게 생성됨

			     (5) 1차 스냅샷으로 복원하기
				0. EC2 서비스 - 인스턴스 목록에서 EC2-STG1 중지시킴
				1. EC2 서비스의 EBS 볼륨 탭의 볼륨 클릭후, 기존 EC2-STG1_Root_volume 에 체크 후 , 볼륨 연결 해제 클릭
				2. EC2 서비스의 EBS 볼륨 탭의 스냅샷 클릭후, 특정 스냅샷 클릭 후 볼륨 생성 클릭
				3. EC2 서비스의 EBS 볼륨 탭의 볼륨 클릭후, 새로 생성된 볼륨 에 체크 후 , 볼륨 연결 클릭
				4. 다음과 같이 설정 후 연결 버튼 클릭
					(1) 연결 인스턴스 >>  EC2-STG1
					(2) 디바이스 이름 >> /dev/xvda1 			
						: 루트 장치에 대한 복원이므로

				5. SSH 접속해 확인해보면 이전 시점으로 돌아갔음 확인 가능 >> df -hT /dev/xvda1
					Filesystem     Type  Size  Used Avail Use% Mounted on
					/dev/xvda1     xfs    20G   13G  7.9G  61% /



	4. EFS 스토리지를 생성하고 사용
		: (EBS 는 같은 가용영역에 속한 인스턴스 하나와만 연결이 가능했지만) EFS 스토리지는 다양한 가용영역에 속한 여러 인스턴스와의 연결이 가능하다

		: EFS 와 VPC, 가용영역, 서브넷 
			: EFS 는 하나의 VPC 에서만 사용 가능
				: 주의 >> EFS 는 여러 가용영역에 걸쳐서 사용할 수 있는거지, 여러 VPC 에 걸쳐 사용할 수 있는게 아니다
					: 한 VPC는 여러 가용영역에 걸쳐 존재 가능
			: EFS 는 여러 가용영역에 걸쳐 사용 가능
				: 해당 EFS 가 속한 VPC 가 여러 가용영역에 걸쳐 존재한다면
 				: 그냥 가용영역 단위로 연결되진 않고, 가용영역 내 특정 서브넷 단위로 해당 가용영역과 EFS 가 연결된다


		: EFS 의 Mount Target(탑재 대상) >> EFS 엔드포인트. 특정 가용 영역(AZ) 내에서 EFS 파일 시스템과 연결할 수 있게하는 지점.	
			: AWS 상에선 Mount Target 이라고 하는데, 통상적으론 걍 엔드포인트라고 많이 함
			: Mount Target 의 생성 시, 이에 대한 ENI도 함께 자동 생성 및 연결된다
				: 이 ENI 는 , 해당 Mount Target 이 속한 서브넷에 내의 ENI 로, Mount Target 이 속한 서브넷 내의 인스턴스와, EFS 시스템이 통신할 수 있게 한다

			: Mount Target 생성시 설정 사항들
				1. (VPC 내의 특정) 가용영역
				2. (해당 가용영역 내의 특정) 서브넷
				3. IP >> Mount Target의 private IP 
					: 자동 할당할 수도 있고, 해당 서브넷 내의 특정 IP를 직접 부여할 수도 있음
					: 물론 사실상 Mount Target 자체의 IP가 아닌, MountTarget 과 연결될 ENI 의 IP 를 의미한다.
					: public IP 를 필요로 하지 않은 이유 >> EFS자체가 VPC 내의 리소스 공유를 목적으로 설계된거다( 외부 인터넷에서까지의 리소스 공유를 가능하게 하는 것은 목적이 아님. 그러니까 일단 어찌됬건 외부인터넷으로 리소스 얻고 싶으면 특정 인스턴스를 통해 EFS 에서 리소스를 얻어야된다) 

				4. 보안 그룹 >>Mount Target 을 통해 EFS 시스템에 접근하는 트래픽을 필터링하는데 사용되는 보안 그룹 지정


		: EFS 의 DNS 
			1. EFS DNS >> <EFS-ID>.efs.<리전>.amazonaws.com
				: 해당 DNS를 조회하는 인스턴스가 속한 가용영역을 담당하는 마운트 타겟의 IP를 값으로 한다
					: 그러니까 조회하는 대상이 어느 마운트 타겟을 거치느냐 따라 DNS 의 IP값이 다르게 나온다


			2. EFS 마운트 타겟의 프라이빗 DNS >> ip-<Private-IP>.<리전>.compute.internal
				: 해당 DNS를 조회하는 인스턴스가 어디에 속해있건,  해당 VPC내에서 DNS 에 명시한 <privateIP>를 private IP로 가지는 마운트 타겟의 IP를 값으로 한다
					: 그러니까 조회하는 대상에 상관없이 DNS의 IP 값이 항상 일정하게 나온다
				: private 하다는건, (private subnet이 외부 네트워크에선 접근 못해도 같은 네트워크에 속한 다른 subnet 에선 접근 가능한 것처럼) 외부 네트워크에선 사용 불가하지만 같은 VPC 내에 속한 서브넷이라면 사용 가능한 DNS 란 의미.
					: 그러니까  같은 VPC 에 속한다면, 다른 가용영역의 EFS mount point 의 IP를 조회할 수 있단 의미
				



		: 절차 
			(1) AWS 콘솔 - 스토리지 - EFS 서비스에서 , "파일시스템 생성" 버튼 클릭, "사용자 지정" 버튼 클릭
			(2) 다음과 같이 설정하여 사용자 지정 EFS 생성
				1. 파일시스템설정
				    (1) 일반  
					1. 이름 : cloudneta
					2. 파일 시스템 유형 : 그대로 (리전)
					3. 자동백업 : 체크되있는거 해제
						: 그러니까 자동 백업을 안시키겠단 의미
					4. 수명주기관리
						(1) Infrequent Access IA 로 전환 : "없음" 선택
							: 그러니까 내가 방문을 얼마나 자주하는지의 패턴 따라서 EFS 의 유형을 IA로는 자동 전환되게 허용하지 않을것이란 의미

						(2) Archive 로 전환 : 그대로("마지막 액세스 이후 90일 경과")
							: 그러니까 내가 90일 이상 방문 안하면 걍 Archive 로 전환되게 허용한다는 의미

						(3) Standard 로 전환 : 그대로("없음")


					5. 암호화 >> "유휴 시 데이터 암호화 활성화" 체크 되있는거 해제
						: 유휴 시 데이터 암호화 활성화 >> 파일 시스템에 저장되는 모든 데이터 대해 자동으로 암호화한다.
							유휴 데이터 Data at Rest >> 시스템에서 이동하지 않고 저장 매체에 저장된 상태로 있는 데이터. 저장되있는 데이터.


						: 그러니까 데이터 저장할 때 자동으로 암호화 시키진 않겠단 의미


				    (2) 성능 >> 선택된거 그대로
					1. 처리량 모드 >> "개선됨"
					2. 탄력적 요금제 체크된 상태로


				2. 네트워크 액세스 설정
					(1) VPC >> 해당 EFS가 사용될 VPC
						: CH5-VPC 선택 


					(2) Mount Targets. 탑재 대상 >> 네트워크 상에서 EFS에 접근 가능하게 하는 지점. 엔드포인트
						: 각 가용영역 AZ 마다 하나 이상의 탑재 대상이 설정됨

						1. 가용영역 >> VPC 내의 가용영역 택
							: 그대로 냅둠 (하나는 ap-northeast-2a, 나머지는 ap-northeast-2c)
						2. 서브넷 ID >> 해당 가용영역 내의 서브넷 선택
							: 그대로 냅둠 (어차피 각 가용영역에 서브넷 하나씩밖에 안만듦)
						3. IP 주소
							: 그대로 냅둠 ( "자동" )
								: 지정된 서브넷 내에서 IP 주소를 선택하는 것도 가능 

						4. 보안 그룹 
							:  두 탑재 대상모두 (기존 default 제거 후) storagelab 선택
					 			: storagelab 은 cloudformation 템플릿으로 생성해둔 보안 그룹


				3. 옵션 및 검토 >> 걍 계속 다음 클릭하고, 생성

 
			(3) 생성된 EFS 스토리지 확인 
				: EFS ID 는 이후 SSH 터미널에서 활용하므로 줍줍하자.
				: 각 가용영역(서브넷) 별 Target Group 에 대한 ENI 가 자동 생성됬음을 확인 가능

			(4) EC2-STG1 인스턴스에 접속하여 EFS 연결
				: 앞선 절차들에선 "공유파일시스템"과 "각 가용영역(서브넷)에서 이 시스템에 접근할 수 있게 하는 엔드포인트" 를 생성만 한거지, 각 가용영역 내의 인스턴스와 엔드포인트 들을 연결해주기까지 했던게 아니다
				1. EC2-STG1 에 접속
				2. 웹 서버 동작 잘되는지 확인	>> curl localhost
					(템플릿에 설정해둔대로) <html><h1>Storage LAB - Web Server 1</h1></html> 가 정상적으로 출력된다
				3. EFS 디렉터리 생성 >> mkdir /var/www/html/efs
					: 그러니까 마운트 포인트 생성

				4.  EFS ID 확인 후 마운트 
					(1) 생성한EFS시스템의ID값을 변수로 저장 >> EFS=생성한EFS시스템의ID값
					(2)  EFS 파일 시스템을 마운트 >> mount -t efs -o tls $EFS:/ /var/www/html/efs
					     : 그러니까 해당 EFS 시스템의 루트 경로를 , 이 서버에서 /var/www/html/efs 를 통해 접근 가능하게 연결하는 과정
						-t efs >> 파일시스템의 타입이 efs 이다
						-o tls >> EFS와의 통신을 TLS(Transport Layer Security)를 사용하여 암호화
						$EFS:/ >>  EFS 파일 시스템을 루트 디렉터리(/)로 마운트
						/var/www/html/efs >> EFS를 마운트할 EC2 인스턴스 내의 디렉터리


				5. EFS 마운트 제대로 됬나 테스트 
					(1)  EFS 마운트한 곳에 파일 생성해보기 >> echo "<html><h1>Hello from Amazon EFS</h1></html>" > /var/www/html/efs/index.html
						: EFS 의 마운트포인트인 /var/www/html/efs 에  "<html><h1>Hello from Amazon EFS</h1></html>" 을 내용으로하는 index.html 을 생성한다.
					(2) 제대로 생성됬나 확인해보기>> curl localhost/efs/
						: "localhost/efs" 가 아니라 정확히 "localhost/efs/"로 해야되는거 주의
						: <html><h1>Hello from Amazon EFS</h1></html> 가 출력된다
							: /var/www/html 파일 아래의 디렉터리명을 path 로 하면, 해당 디렉터리의 index.html 이 반환되는 듯 

				6. EFS 사이즈 확인 >>  df -hT | grep efs
					: df -hT >> 디스크 사용량을 보기 쉽게 출력
					: grep efs >> EFS에 해당하는 라인만 필터링
					: 출력 결과 >> 127.0.0.1:/    nfs4      8.0E     0  8.0E   0% /var/www/html/efs
					: EFS 는 크기 제한이 없어 EFS 에 마운트된 파일 시스템은 항상 최대 용량으로 표시된다 && 비용은 사용 용량에 따라 부과된다

				7. EFS의 DNS 주소를 조회하여, IP 주소를 확인 >>  dig +short $EFS.efs.ap-northeast-2.amazonaws.com
					: dig +short >> DNS 정보를 조회하되, 결론(해당 도메인네임 대한 IP값)만 짧게 표시
					: $EFS.efs.ap-northeast-2.amazonaws.com >> EFS 시스템 자체의 ID를 기준으로 한 DNS 로, 조회하는 인스턴스가 속한 가용영역에 대한 IP를 반환한다
					: 결론적으론 다음이 출력됨 >> 10.40.1.135 
						: 그러니까 EC2-STG1 이 사용하는 Mount point 의 IP가 출력된다


			(5) EC2-STG2 인스턴스에 접속하여 EFS 연결
				1. EC2-STG2 에 접속
				2. 웹 서버 동작 잘되는지 확인	>> curl localhost
					 <html><h1>Storage LAB - Web Server 2</h1></html> 가 정상적으로 출력된다
				3. EFS 디렉터리 생성 >> mkdir /var/www/html/efs
				4.  EFS ID 확인 후 마운트 
					(1) 생성한EFS시스템의ID값을 변수로 저장 >> EFS=생성한EFS시스템의ID값
					(2)  EFS 파일 시스템을 마운트 >> mount -t efs -o tls $EFS:/ /var/www/html/efs

				5. EFS 마운트 제대로 됬나. 공유 잘 되나. 테스트 >> curl localhost/efs/
					: <html><h1>Hello from Amazon EFS</h1></html> 가 출력된다
						: 이전에 EC2-STG1 에서의 작업 내용이, EC2-STG2 를 통하여도 접근이 가능함을 알 수 있다

				6. EFS 사이즈 확인 >>  df -hT | grep efs
					: 출력 결과 >> 127.0.0.1:/    nfs4      8.0E     0  8.0E   0% /var/www/html/efs


				7. EFS의 DNS 주소를 조회하여, IP 주소를 확인 >>  dig +short $EFS.efs.ap-northeast-2.amazonaws.com
					: 결론적으론 다음이 출력됨 >> 10.40.2.79 
						: 그러니까 EC2-STG2 가 사용하는 Mount point 의 IP가 출력된다


			(6) 인스턴스 간 EFS 로 파일 공유가 잘 되는지 확인
				1. EC2-STG1 에서 반복문으로 (마운트포인트에) 파일 100개 생성 >> for i in {1..100}; do touch /var/www/html/efs/deleteme.$i; done;
						: deleteme.1, deleteme.2 , .. , deleteme.100 의 파일이 생성된다

				2. EC2-STG2 에서 
					(1) 마운트포인트의 내용물 확인해보기 >> ls /var/www/html/efs 
						: deleteme.1, deleteme.2 , .. , deleteme.100 의 파일을 확인 가능

					(2) 마운트포인트의 파일 100개 삭제 >> rm -rf /var/www/html/efs/deleteme*.*


				3. EC2-STG1 에서 마운트포인트의 내용물 확인해보기 >> ls /var/www/html/efs 
					: 앞서 있었던 deleteme1, deleteme2 , .. 의 파일이 정상 삭제되어있다




	5. Public S3 스토리지로 외부 접근 확인 
		: ACL  과 security group 복습
			: ACL Network access control list >> 서브넷 수준의 방화벽 
				: 그러니까  라우팅 테이블을 지나 서브넷으로 들어가고 나가는 트래픽을 제어하는 역할
			:  security group >> 인스턴스 수준의 방화벽
				: 그러니까 인스턴스에서 들어가고 나가는 트래픽을 제어하는 역할
			: https://martinkim1954.tistory.com/entry/AWS-ACL-%EB%B0%8F-%EB%B3%B4%EC%95%88%EA%B7%B8%EB%A3%B9-%EB%B9%84%EA%B5%90-%EC%83%9D%EC%84%B1-%EB%B0%8F-%EC%A0%81%EC%9A%A9

		: S3 >> 객체 스토리지 서비스. 
			: 설정 따라 OS 없이 자체적인 웹 서버로 사용되거나 API 방식을 이용한 데이터 백업 및 공유 가능
			: 외부 접근 가능 >> public IP를 가질 수 있다 ? 특정 VPC 에 종속되지 않고?

		: S3 버킷명>>  전세계 적으로 고유해야한다. 계정, 리전 상관없이 이미 해당 이름의 S3 버킷이 존재하는 경우 해당 이름으로는 버킷 못 만든다. 
			: 그러니까 다른 사람이 베이징 리전에 cloudneta 란 버킷을 만들어 사용하고 있는 상태이면, 서울 리전에서도 cloudneta 란 이름으론 버킷을 못 만든단 말이다
			: 왜 굳이 S3 버킷명을 전세계 적으로 고유하게 한걸까>> S3 버킷은"https://<bucket-name>.s3.amazonaws.com/ " 의 형식으로 전세계에서 접근 가능하다. 각 버킷 별로 URL을 통한 접근을 가능하게하기 위해선 bucket-name이 유일해야된다.


		; S3 버킷 
			: 생성시 설정하게 되는 내용들
				(1) 일반구성
					0. AWS 리전 >> 버킷이 생성될 리전 
						: 현재 AWS 콘솔의 리전값으로 자동 설정되고, 직접 선택 불가하게 되어있다. 다른 리전에서 생성하고 싶으면 AWS 콘솔에서 해당 리전으로 전환 후 버킷을 생성함 된다.

					1. 버킷 이름 
						: 주의 >> S3 버킷명은 전세계 적으로 고유해야한다.
							: 일일이 이미 존재하는 이름인지 찾아볼 필욘 없고 생성 버튼 눌렀을 때 "이미 존재하는 이름입니다" 안뜨면 고유한 이름이고, 통과된 것.

				(2) 객체 소유권 >> S3 버킷에 업로드된 객체(파일)에 대한 소유권 설정
					: 버킷 소유자와 객체 업로더 간의 소유권을 어떻게 할당할지를 결정하게 됨
					: 종류
						1. ACL 비활성화됨(권장)
							: ACL을 사용하지 않으므로, 버킷 정책을 통해서만 액세스를 제어할 수 있음
							: 권장되는 이유는 보안 및 관리가 간소화되기 떄문. 
								: ACL을 활성화해야 할 특별한 이유가 없다면(=객체 업로드한 사용자가 소유권을 가지게 해야하는경우가 아니라면), ACL을 비활성화하는게 좋음
							(1) 모든 객체의 소유권을 버킷 소유자에게 할당

						2. ACL활성화됨. 
							: ACL를 사용하여 세부적인 권한 관리를 가능하게 함 (다른 AWS 계정이 객체를 업로드하고 소유할 수 있음)
							(1) Bucket owner preferred 버킷 소유자 선호 >> 모든 객체의 소유권을 버킷 소유자에게 할당

							(2) Object writer 객체 라이터 >> 객체를 업로드한 사용자에게 해당 객체의 소유권을 할당

				(3) 이 버킷의 퍼블릭 액세스 차단 설정 

		: 절차 
			(1) 버킷 생성
				1. AWS 콘솔에서 S3 서비스의 버킷 탭을 클릭, "버킷 만들기" 버튼을 클릭
				2. 다음과 같이 설정하여 버킷 생성
					(1) 일반구성
						1. 버킷 이름 : my-cloudneta
							: 주의 >> S3 버킷명은 전세계 적으로 고유해야한다. 내가 아닌 다른 계정이라도, 이미 해당 이름의 S3 버킷이 존재하는 경우 해당 이름으로는 버킷 못 만든다. 
							: 교재에서는 cloudneta 로 생성했던데, 어떤 사람이 이미 사용중인지 해당 이름으론 못만든다 떠서 my-cloudneta 로 대신 생성함

					(2) 객체 소유권 >> ACL 활성화됨
						: 디폴트 체크되있는 ACL 비활성화됨 선택 안한거임 주의

					(3) 이 버킷의 퍼블릭 액세스 차단 설정 >> (디폴트로 체크되있는) 모든 퍼블릭 액세스 차단을 선택 해제하고, 밑의 (경고란의) "현재 설정으로 인해 이 버킷과 그 안에 포함된 객체가 퍼블릭 상태가 될 수 있음을 알고 있습니다." 에 체크 표시

			(2)	
	6. Private S3 스토리지의 제한된 접근 및 데이터 백업
	7. 생성된 자원을 모두 삭제
