4.1 Amazon ELB

OSI 7 계층 관련
	: 계층이 높아질수록 이전 계층을 기본적으로 깔고가면서 +α 시킨다는 느낌
		: ex )  ARP 가 MAC 을 사용하긴 해도, IP 까지 사용해야해서 2계층이 아닌 3계층에 속함

	:  아무리 더 높은 계층이 기본적으로 그 아래의 계층을 활용한다고 해도, 일반적으로는 그 아래 계층에서는 작동 불가 >> 그 아래 계층 이상의 계층에 속해있다는건, 그 아래 계층 이상의 무언갈 필요로 했단 말이니까. 
		: 그러니까 자신이 속해있는 계층에서만 작동 가능

	: HTTP 는 7계층이었다!


레거시 legacy >>  낡은 기술. 
	: 남아있긴 하지만 더이상 선호되진 않는.

로드 밸런싱 load blanancing , 부하분산 >> 서버 - 클라이언트 환경에서, 서버가 클라이언트 요청을 받아 처리하는 과정에서 발생하는 부하(연산작업)에 대해 동일한 목적을 수행하는 다수의 서버에 분산 처리하는 기능	
	: 간단하게 말하면, 트래픽을 (서버로) 효율적으로 배분하는 기능.
	: "부하분산" == "로드 밸런싱" . 같은 뜻.
	: 부하분산을 사용 효과 >> 고가용성, 내결함성이 향상 
		고가용성 High availability >>시스템을 항시 사용 가능하게 하는 기능
		내결함성 Fault tolerance >>시스템 일부에 결함이 있더라도 계속 작동할 수 있게 하는 능력

로드 밸런서 load balancer >> 부하분산을 실행시키는 대상




Amazon ELB Elastic Load Balancing : Amazon EC2 인스턴스에서 운영중인 애플리케이션, 서비스로 유입되는 트래픽을 자동 분산 처리하는 기술
	: 여러 가용영역에서 작동
	: 지원
		: HTTP(S) , SSL 등 다양한 프로토콜
		: SSL 암호화 
		: 사용자가 같은 인스턴스에서 세션을 유지 가능
		: 네트워크 및 어플리케이션 수준의 로드밸런싱

	: 활용 방안 >> 다른 AWS 서비스와 짬뽕하여 유용하게 쓰인다
		: AWS 의 Cloudwatch 와 함께 >> 로그와 메트릭을 모니터링 
		: AWS 의 Autoscaling 과 함께 >> 트래픽이 증가할 때 자동으로 인스턴스를 추가하거나 제거하면서 애플리케이션 가용성을 유지

	: 원리 
		(0) 클라이언트가 ELB 의 DNS 주소로 트래픽을 전달(= request 를 보냄)
		(1)  로드 밸런서에서 클라이언트 request 를 수신받고, 클라이언트와 연결을 유지한 상태에서, request 를 수신하기 위해 리스너를 "등록"

		(2) 로드밸런서가 수신한 클라이언트 request 를 처리할 대상 그룹을 선택.

		(3) 로드밸런서가 ( 앞서 선택한 "대상 그룹" 내에서 ) request 를 처리할 대상을 딱 특정하고, 해당 대상으로 request 를 분산(=전달). 
			: 로드 밸런서는 각 대상의 상태를 모니터링하고 있어. 가용하지 않은 대상에겐 request 를 분산시키지 않음.
		
		(4) 분산된 request 대해 대상이 반환한 response 를 로드밸런서가 클라이언트에게 반환
 



	: 주요 구성 요소
		: 로드 밸런서는 가용영역 단위로, 리스너는 리전 단위로 배포함으로써 ELB가 각 가용영역으로 트래픽을 분산시키는걸 가능하게 한다.

		1. 로드 밸런서 Load Balancer >> 클라이언트와 직접적으로 소통( request 받고 response 전달 )하는 역할 + 내부적으론 클라이언트로부터 받은 request(트래픽)를 (상태가 정상인) 대상 그룹에 정의된 서버로 분배하는 역할
			: 일종의 라우터.
			: 가용영역 단위로, 로드 밸런서 "노드" 로써 배포된다.
			: 종류 >> 트래픽의 프로토콜종류, 서비스 목정, 대상 등 따라 4가지의 로드 밸런서를 제공
				(1) CLB Classic Load Balancer >> 가장 초기에 출시된 로드 밸런서
					: 잘 안쓴다. 레거시.
						: 서버의 주소가 변경되면 로드 밸런스를 새로 생성해야됨
						: 기능적 한계 (포트, 헤더 등을 수정 불가)
					: 4계층, 7계층 ( Application layer ) 로드 밸런서		
					: 7계층 로드 밸런싱 >>	 HTTP/HTTPS 요청의 내용을 기반으로 트래픽을 분배	
							: 프로토콜 >> HTTP/HTTPS 지원

						: 4계층 로드 밸런싱 >> IP 주소와 포트 번호를 기준으로 트래픽을 분배
							: 프로토콜 >> SSL/TLS 지원


				(2) ALB Application Load Balancer >> 웹 어플리케이션(HTTP/HTTPS)에 특화된 로드 밸런서
					: 7 계층 로드 밸런서 >> HTTP/HTTPS 지원. 
					    : HTTP/HTTPS 프로토콜 기반 다양한 라우팅 기능 제공
						1. URL path 기반 라우팅 
						2. host 기반 라우팅
						3. 쿼리 스트링 기반 라우팅 

					: 람다를 대상 그룹 지정 가능

				(3) NLB Network Load Balancer >> 대규모 트래픽처리와 빠른 응답에 특화된 로드 밸런서
					: 4 계층 로드 밸런서 >> TCP, UDP, SSL/TLS 프로토콜 지원
						: 신뢰성,안정성 등을 이유로 (아무리 대규모라해도) UDP 보단 TCP사용
						: 7계층인 ALB에 비해선 저수준 >> 비교적 빠르고 & 대규모 트래픽을 처리하는데 용이. (= 성능이 좋다)

					: (일반적인 로드 밸런서완 달리) 클라이언트 IP 주소를 원래 IP 주소로 보존 가능
						: 일반적인 로드 밸런서 >> 중간에 로드 밸런서가 클라이언트의 트래픽을 받으면, 클라이언트의 IP 주소를 로드 밸런서 자신의 IP 주소로 대체한다. 

						: NLB에서 이 기능은 특별한 설정 없이 기본적으로 제공 << 이는 NLB가 레이어 4(전송 계층)에서 작동하기 때문
						: 사용자 분석을 해야하는 경우 유용

					: (4가지 로드 맬런서 중 유일하게) 고정 IP 사용 가능

					: 주로 게임 서버, VolP 서비스, 미디어 스트리밍 등에서 사용

			


				(4) GWLB GateWay Load Balancer >> 네트워크 트래픽을 (중간에 거치는, 경유용) 다른 네트워크의 대상 그룹(서드파티)으로 부하분산 처리하는 로드 밸런서
					: 주로 보안 강화 목적으로 사용
					: 3계층 , 4계층 로드 밸런서 >> IP 주소기반 라우팅 과 TCP, UDP 프로토콜 지원
					: GWLB, GWLB Endpoint, GWLB Endpoint Service 
						: GWLB 를 사용하기 위해선 GWLB Endpoint, GWLB Endpoint Service 를 추가적으로 사용 필수
			
						: GWLB >> GWLB Endpoint Service와 서드파티, 서드 파티와 GWLBE 간의 통신을 중계하는 역할
							: GWLB 자체는 다른 네트워크 내부에, 서드파티와 함께 있음

						: GWLB Endpoint Service >> 클라이언트와 GWLB 간의 통신을 중계하는 역할.
							: 클라이언트에게 직접적으로 request 를 받고, response 를 하게 된다. 
							: GWLB 와 함께 다른 네트워크 내부에 있음
						
						: GWLBE GWLB Endpoint >> GWLB와 (실제 목적지로 하는) 대상 그룹간의 통신을 중계하는 역할
							: 실제로 서비스를 제공하는 대상 그룹과 함꼐 있음 


					: https://kim-dragon.tistory.com/167
					: https://choiblog.tistory.com/169


		2. 대상 그룹 Target Group >> 로드 밸런서로부터 트래픽을 분배당할 하나 이상의 엔드포인트(예: EC2 인스턴스, Lambda 함수, IP 주소 등)를 정의하는 그룹
			: 일종의 (트래픽을전송받는) 네트워크 장치 그룹.


		3. 리스너 Listener >> 로드 밸런서가 받은 request 대해 적용할 rule 과 그에 대한 action(=그 규칙을 만족하는 request 일 경우 해당 request 를 어디로 라우팅 시킬건지) 을 정의 
			: 일종의 라우팅 테이블
			: 리전 단위로 배포된다.
			: 허용 가능한 포트와 프로토콜 등을 정의하여, 특정 대상 그룹으로 트래픽을 라우팅할 수 있게 해준다
				: 다양한 프로토콜을 지원



	: ELB 를 생성할 때 로드 밸런서와 통신하는 방식
		: 인터넷 경계 로드 밸런서 >> 외부에서 직접 로드 밸런서에 접근하는 방식
		: 내부 로드 밸런서 >> 외부의 접근이 차단된 격리된 네트워크(=내부 네트워크)에서 로드 밸런서를 사용하는 방식

	: ELB 교차 로드 밸런싱 Cross-Zone Load Balancing 
		: 트래픽 분배를 인스턴스 단위로 n빵해준다
		: 활성화 시킬 수도 있고, 비활성화 시킬 수도 있다
			: 교차로드밸런싱기능이 비활성화된 경우, 각 (상태가 정상인) 가용영역 내에 인스턴스가 몇개냐 그런거 전혀 고려 없이 가용영역 단위로 n빵 시키게된다. 
			: 반면 교차로드밸런싱기능이 활성화된 경우, (상태가 정상인) 가용영역 들 내의 인스턴스들 단위로, 고르게 n 빵 시켜준다.
			: 어떤 서비스를 사용하느냐 따라서 교차 로드 밸런싱 기능이 디폴트로 활성화 되있기도 하고 비활성화 되있기도 하다
		: 기본적으로는 "로드 밸런서 수준"에서 설정하고, (필요하다면) 세부적으로는 "대상 그룹 수준"에서 설정한다
			: 로드 밸런서 수준에서의 설정 >> 교차 로드 밸런싱 기능 자체를 활성화 할건지의 여부. 그러니까 걍 가용영역 단위로 n빵할건지, 대상 그룹 내부의 인스턴스 단위로 n빵할건지를 결정.

			: 대상 그룹 수준에서의 설정 >> (해당 대상그룹이 트래픽을 받는다면) 각 인스턴스가 어느 정도의 비율로 받게 될 건지를 설정. 


-----------------------------------------------------------------------------------------------------

4.2 ALB 와 NLB 를 이용한 로드 밸런싱 구성

프로비저닝 provisioning : (필요할 때 바로 사용가능하도록) 준비해두는 것.
	: https://jake-seo-dev.tistory.com/210


IaC Infrastructure As Code : (수동으로 자원을 만들지 않고) 코드를 통해 자원/인프라를 제공.생성


DevOps


배포 과정 :
	1. 개발자가 코드를 원격 저장소에 올림
	2. 원격 저장소에 올라간 코드가, 아래의 과정을 모두 통과
		(1) build
		(2) test
		(3) release

	3. (모두 통과하고) 빌드된 형태로 배포 서버에 전달됨
	4. 배포 서버가 애플리케이션 서버에 최종 배포를 함

배포 자동화 : 한번의 클릭 혹은 명령어 입력을 통해 전체 배포 과정을 자동으로 진행하는 것


파이프라인 Pipeline : 소스 코드의 관리부터 실제 서비스로의 배포 과정을 연결한 구조.
	: 파이프라인은 전체 배포 과정을 여러 Stage 로 분리
		1. Source 단계 : 원격저장소에 관리되고 있는 소스 코드에 변경 사항이 일어날 경우 이를 감지하고 다음 단계(Build 단계)로 전달.
		2. Build 단계 :  코드를 컴파일/빌드/테스트 및 빌드된 형태로 다음단계(Deploy단계)로 전달.
		3. Deploy 단계 : 빌드를 실제 서비스에 반영

	: ( CI/CD ) 파이프라인을 구축한다 == 배포 자동화 시스템을 구축한다
		CI/CD 
			: CI Continous Integration >> 지속적 통합.
				: 코드의 변경 사항이 자동으로 빌드 및 테스트 되어 애플리케이션에 반영된다
			: CD Continouse Delivery/Deployment >> 지속적 제공/배포
				: (변경사항이 있어 새로 빌드 .. 등 하고 배포될 준비가 되면) 자동으로 배포된다
	: https://velog.io/@edith_0318/CICD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8


TCP/UDP + 특정번호 == 특정 프로토콜
	TCP 22 : SSH
	TCP 80 : HTTP
	UDP 161 : SNMP


TCP 와 UDP 느낌 >> 일단 둘 다 네트워크 상으로 데이터를 전송하는 경우 쓰이는 프로토콜. 
	: TCP >> 보안. 신뢰성이 좀 필요한 경우
	: UDP >> 걍 요청-응답만 구현하면 됬지, 그다지 신뢰성이 중요하진 않은 경우


HTTP HyperText Transfer Protocol >> 인터넷서 데이터 주고 받을 수 있게하는 표준 프로토콜
	: 웹 서버 - 클라이언트 구조 , request 보내고  response 반환
	: 다양한 메서드 사용 >> GET , POST, PUT , DELTE
	: 80 번 포트 사용
	
Simple Network Management Protocol >>네트워크를 통해 각 네트워크장비들을 모니터링하고 관리할 수 있게하는 프로토콜 
	: SNMP 자체는 "프로토콜"일 뿐, 이를 활용하여 실제 각 네트워크 장비의 정보를 얻기 위해선 이를 이용하고자하는 장비쪽에 별도의 프로그램을 설치 필요
		:  agent 측에는 SNMP Daemon 과 같은, 해당 agnet 와는 전혀 별개의 소프트웨어가 돌아가고 그 소프트웨어가 SNMP 프로토콜을 사용한 통신을 가능하게 하는 거다.

	: SNMP 구성 요소 
		: agent >> 관리 대상. 정보를 제공하는 하드웨어/소프트웨어 
			: 161 번 UDP 포트 사용 , 예외적으로 TRAP 타입 메세지를 전송하는 경우 162번 UDP 포트 사용

		: manager >> 관리시스템/관리자. 정보를 수집하는 하드웨어/소프트웨어 
			: 162번 UDP 포트 사용
			: 현업에서는 manager 을 NMS Network Management System 이라고도 부른다.
	
 	
	: 동작 방식 >> SNMP 관리자와 agent 간에 메세지를 주고 받음
	: ( agent 와 manage 간 ) 통신을 위해선 UDP를 사용, SNMP 자체는 7 계층 프로토콜

	: 버전 >> SNMPv1 , SNMPv2c, SNMPv3 ... 등 계속 업뎃됬음
	     : 주로 Community String 에 대한 보안 강화를 위한 업데이트
		SNMPv1: SNMP의 첫 버전. 보안이 약해서 잘 안 씀 ( 모든 데이터는 평문 )
		SNMPv2/SNMPv2c: v1에서 보안 문제를 좀 개선
		SNMPv3: 최신버전. Username 과 password 를 넣고, 더 강력한 암호화 적용.
		
	: SNMP Message 의 타입
		: Http 메서드 느낌
		(1) SNMP Get : (관리자 쪽에서 보내는) 장비 정보 조회 메시지

		(2) SNMP Set  : (관리자 쪽에서 보내는) 장비 정보 수정 메시지
			: 잘 사용되진 않음. (네트워크 장비 설정은 주로 네트워크 엔지니어가 직접 변경)

		(3) SNMP TRAP : "에이전트 쪽에서 보내는" 메시지
			: 관리자 쪽에서 요청하지 않아도, 이상 상황 발생시 장비가 보내는 메시지 타입. 관리자가 놓칠 수 있는 부분을 보완하는 역할

 

	: 관련 용어
		: Community String 커뮤니티 값 >> agent 와 manger 간의 인증을 위한 값
			: manager 와 agnet 가 서로 동일한 community string 값을 가지고  있어야 통신이 성립하게됨
			: Community String 속성의 종류
				(1) read-only >>에이전트가 매니저로부터 get request 만 받을 수 있다는 것. (그러니까 단순 조회에 대한 응답만 가능)
				(2) read/write >> 에이전트가 매니저로부터 get request 뿐 아니라 set request 도 받을 수 있다는 것 (조회 뿐 아니라 수정에 대한 작업 및 응답이 가능) 

			: 별다른 설정 없을 시 디폴트론 read-only 속성의 "public"이란 문자열이 Community String 으로 지정된다.
			: https://m.blog.naver.com/aepkoreanet/222101737343

		: Object >> 데이터덩어리. agent 와 manager 가 주고 받은 SNMP message 정보
		: OID Object ID >> Object 의 고유 식별자. 
			: SNMP 에서는 각 네트워크 장비의 기능, 설정이 OID 를 기준으로 구별됨
			: 표기 >> 숫자가 '.'을 기준으로 구분되어 나열된 꼴
				: 사실은 OID 는 트리 구조이고, 이를 '.' 을 기준으로 나열해 표기한 것. 
					: OID 앞쪽 일수록 트리 상단부 노드 , 뒤로 갈수록 말단부 노드
					: 각 노드는 특정한 의미를 가진다 (ex: 특정 벤더, 특정 장비 기능)
				: 그러니까 OID 는 의미 없는, 단순 구별하기 위한 식별자가 아니라, 의미가 있는 일관된 식별자이고 (OID를 통해 벤더와 , 해당 장비의 기능 등을 파악 가능), 그렇다고 이걸 외우는 사람은 ㅂㅅ 인거고 걍 검색해라 . 그때그때.
			: 종류
				(1) Public OID >> 벤더 공통적으로 사용되는 OID 
					: System, Interface, IP 등 벤더 상관없이 필수적인 내용을 제공하는 OID

				(2) Private OID >> 벤더별로 지정되어 사용되는 OID 

		: MIB Management Information Base >>  Object 들의 집합. db의 테이블이라고 봄 됨.
			: 그러니까 쉽게 비유하면
				MIB = 테이블
				Object = 테이블의 row
				OID = 테이블 row 의 ID 컬럼값
 
	: https://aws-hyoh.tistory.com/179



기본적인 DNS 의 원리
	: https://yaelimeee.tistory.com/46

CPU 라운드 로빈 Round Robin >> 프로세스 사이에 우선순위를 두지 않고, 순서대로 CPU를 할당하는 방식의  CPU 스케줄링 기법

DNS Round Robin >> DNS 서버 구성 방식 중 하나로, 도메인네임에 대해 여러 서버(의 IP)가 존재할 때, 순서대로, 돌아가면서, 트래픽을 각 서버에게 할당하는 방식
	: https://yaelimeee.tistory.com/46
	


ENI Elastic Network Interface >> 탄력적 네트워크 인터페이스.
	: 일종의 가상 LAN 카드
		: 랜카드 >> 컴퓨터를 네트워크에 연결하는 역할을 하는 부품.

	: 포함 정보. 생성시 설정하게되는 정보
		0. subnet >> 해당 ENI 가 생성될 서브넷
		1. IP 
			(1) private IP
			(2) public IP <-- optional
				: 리소스에 public IP 를 할당할 경우 가지게 된다.
		2. MAC address
		3. 보안 그룹

	: 인스턴스와 ENI
		: 얘가 실질적으로 인스턴스와 인터넷,서브넷, 보안 그룹을 연결시킨다 
			: 사실 인스턴스는 해당 서브넷 내부에 생성되있는게 아니라 그런 것처럼 ENI 가 연결하는거임 + 인스턴스 직접적으로 public IP를 가지는게 아닌데 그런 것처럼 ENI가 제공해주는거임

		: EC2 인스턴스는 반드시 하나 이상의 ENI 와 연결되어있어야된다 
		: ENI 는 인스턴스와 별도로 생성 가능하다
			: 미리 ENI 만들어놨다가 특정 인스턴스 생성할 때 해당 ENI 부착했다가, 다시 그 ENI 떼서 다른 인스턴스에 부착시킬 수 있다. 

	:  로드밸런서와 ENI
		: 로드 밸런서 "노드" 별 로도(= 가용영역 당) ENI 가 자동 생성되고, 각 ENI 에 public IP 가 할당되며 특정 서브넷과 연결된다.
	: https://jibinary.tistory.com/133	
	: https://kimjingo.tistory.com/197


셸 변수
	: 프롬프트 창에 단순히 "변수명=값" 을 입력하여, "해당 셀 세션에서" 일시적으로 사용할 변수를 등록 가능하다
		: 세션이 종료되면 메모리에서 해당 변수는 제거된다. ( SSH 접속이 끊길 때마다도 이전에 설정했던 변수를 사용 불가)	
			: 그렇다고 history 상으로 입력한 명령어 내역에서 까지 사라지는건 아니고, 다만 변수로써의 유효력이 없어지는 것.
		: 셸 변수의 영구적인 저장을 원하면(=새로운 셸 세션이 시작될 때마다 변수가 자동 설정되게 하려면) .bashrc 와 같은 초기화 스크립트에  "export 변수명=값" 꼴로 추가해야한다. 
			: 그리고 이를 설정 후 바로 반영시키려면 source ~/.bashrc

	: 등록한 변수를 사용하는 법 >> $변수명
		: 해당 변수뒤에 바로 다른 문자열 써도 잘 인식된다
			ex ) EC21=13.125.22.69 한 상황에서  $EC21/dev/ 은 13.125.22.69/dev/




리눅스 툴 관련
	curl >> Client URL . URL 을 사용해서 서버에 데이터를 request 하는,클라이언트 역할을 하는 툴.
		: 형식 >> curl [옵션] [URL]
		: 자주 쓰이는 옵션
			-s, --silent : 부가 정보 없이 조회
				: ex) curl -s www.daum.net

			-X : 요청시 사용할 HTTP 메서드의 종류(GET POT PUT PATCH DELTE)를 뒤에 명시 
				: ex ) curl -X GET  http://localhost:8080/user/100

			-d : HTTP POST 요청 데이터 입력

			-H : 전송할 헤더를 지정
				: POST 의 기본 Content type 이 JSON 이 아니라서, JSON 파일을 첨부해보내고 싶으면 별도로 헤더 추가 필수
				: ex ) curl -d '{"key1":"value1", "key2":"value2"}' -H "Content-Type: application/json" -X POST  http://localhost:8080/user/100



		: https://velog.io/@odh0112/Linux-Curl-%EB%AA%85%EB%A0%B9%EC%96%B4


	dig Domain Information Groper >> DNS 에게 질의할 수 있는 툴
		: 그러니까 DNS 에게 해당 도메인의 실제 주소가 뭔지 request 하고, 그에대한 response 를 받을 수 있게 해주는 클라이언트 역할을 해주는 툴이란 것
		: 형식 >> dig [@특정네임서버] [도메인네임] [+옵션명] 
			: +옵션명
				(1) +short : 알잘딱깔센. 질의한 결과(해당 DNS 대한 실제 IP값)만 표시함. 
				(2) +trace 
			: 보통 "dig 도메인네임 +short " 의 꼴로 가장 많이 활용한다. 
		: https://carpfish.tistory.com/entry/DNS-dig-%EB%AA%85%EB%A0%B9%EC%96%B4-%EC%86%8C%EA%B0%9C-%EB%B0%8F-%EC%82%AC%EC%9A%A9%EB%B2%95



html과 자바스크립트와 PHP
	: html >> 요청하면 걍 준다. 정적 웹 페이지
	: 자바스크립트, php >> 별도로 실행하고 전달된다. 동적  웹페이지
		: 자바스크립트 >> 클라이언트 사이드 스크립트. 클라이언트에게 전달된 후에 실행되고 결과 확인
		: php >> 서버 사이드 스크립트. 서버 쪽에서 실행되고 클라이언트에게 전달됨.
			: 민감한 정보를 포함해야하는 경우 유용

CloudFormation >> AWS 가 제공하는 IaC 서비스
	: 즉, 실습 환경을 "코드" 기반으로 AWS 인프라 리소스( VPC, EC2 등 )를 자동으로 생성하는 기술
	: 지속적 배포를 원하는 CI/CD 파이프 라인과 통합하여 인프라 관리 자동화도 가능
	: 주요 구성 요소
	    : 템플릿을 해석하여 스택을 생성하고, 정의된 AWS 인프라를 생성/변경/삭제
		1. 템플릿 >> AWS 인프라를 정의하는 JSON 또는 YAML 형식의 파일 
			: 일종의 설정 파일.
			: 인프라의 속성, 관계, 종속성 등을 정의 가능 <-- 템플릿 덕분에 종속성 관리 ㅈㄴ 편하다고 한다
			: 문법
				1. 파라미터 : 템플릿의 윗부분에 정의된 변수로, 템플릿의 재사용성을 높임
					!Ref 을 사용해 파일 내부에서 해당 값 사용 가능	

			:  프로비저닝을 할 수 있다?


		2. 스택 >> CloudFormation 의 관리 단위로, 리소스들의 묶음
			: 스택 삭제 시 해당 스택에 속한 모든 인프라도 함께 삭제되는 거임

		3. 리소스 >> CloudFormation으로 생성한, 말 그대로 리소스. (EC2 인스턴스 , Amazon S3 버킷 등..) 
			

		4. 이벤트 >>  CloudFormation 의 "스택"에서 발생하는 모든 이벤트(생성/변경/삭제 등)
			

	: 작동 방식
		1. 템플릿 작성
		2. 해당 템플릿을 CloudFormation 서비스에 업로드
		3. CloudFormation서비스는 해당 템플릿에 따라 스택을 생성하거나 업데이트 함
			; 스택 모니터링 가능 (생성 또는 업데이트 다른 이벤트. 로그 확인 가능.) 





실습 절차
	1. 기본 인프라를 CloudFormation 으로 배포
		: 절차
			1. 서비스 - CloudFormation 서비스택 - "스택생성" 버튼 클릭
			2. 템플릿 업로드 >> (기존 템플릿 선택 그대로 체크된 채로 냅두고) 템플릿 지정코너에서 Amazon S3 URL 선택 된채로, URL 을 입력 : 교재에서 제공해준 https://cloudneta-aws-book.s3.ap-northeast-2.amazonaws.com/chapter4/elblab.yaml 
					: CloudFormation 템플릿, 즉 yaml 파일을 다운받을 수 있는 URL 임
					: 해당 파일에 정의된 내용
						(1) Parameters 
							1. keyname: >> EC2 인스턴스에 SSH 접근하기 위해 사용하는 기존의 EC2 키 페어 이름
							2. LatestAmiId: >> 어떤 AMI로 EC2 인스턴스 접근할건지 정의

						(2) Resources >> CloudFormation을 통해 생성할 리소스들
							1. VPC 및 기타 네트워크 리소스들
								(1) VPC
									1. ELBVPC
									2. MyVPC

								(2) IGW
									1. ELBIGW
									2. MyIGW
			
								(3) IGW 에 대한 attachment
									: 단순히 IGW 를 생성한다고 끝이 아니라 특정 VPC 에 attach 하는 과정이 필요
									1. ELBIGWAttachment
									2. MyIGWAttachment

								(3) 라우팅 테이블
									: 이게 라우팅 테이블 정의부가 먼저 나왔긴 한데, 이후엔 결국 VPC 내의 서브넷과 연결되어 사용된다

									1. ELBPublicRT
									2. MyPublicRT

								(4) 라우팅 테이블에 IGW 경로 등록
									1. ELBDefaultPublicRoute
									2. MyDefaultPublicRoute

								(5) 서브넷 정의
									1. ELBPublicSN1
									2. ELBPublicSN2
									3. MyPublicSN


								(6) 서브넷과 라우팅 테이블을 연결
									: 그냥 라우팅 테이블을 생성한다고 서브넷에 연결되는게 아님
									: 라우팅테이블은 2개인데 서브넷은 3개라는 점에서 눈치챌 수 있듯이, 같은 VPC 에 속하는 ELBPublicSN1 와 ELBPublicSN2는 같은 라우팅 테이블을 공유한다. (그러니까 서브넷 별 고유한 라우팅 테이블을 가지게 설정한진 않았다)
									1. ELBPublicSNRouteTableAssociation
									2. ELBPublicSNRouteTableAssociation2
									3. MyPublicSNRouteTableAssociation


							2. 보안 그룹 >> EC2 인스턴스에 적용 가능한 보안 그룹을 정의
								(1) MySG >> SSH, ICMP 허용하게 설정됨
								(2) ELBSG >> HTTP, SNMP, SSH, ICMP 허용하게 설정됨

							3. EC2 인스턴스	
							     : Tags 필드를 통해 name 을 설정 가능
								   : KeyName 필드는 키페어 파일명 받는 필드임

							     : UserData 필드를 통해 초기 설정 내용을 명령어로 설정가능- 실습에서는 이를 통해 필요한 툴 및 파일을 미리 설치해놓음
								
								(1) MyEC2 
									: MyVPC에 배포될 t2.micro 인스턴스

								(2) ELBEC21, ELBEC22, ELBEC23
									: ELBVPC 내부 서브넷에 배포될 세 개의 t2.micro 인스턴스
										: ELBPublicSNRouteTableAssociation에는 ELBEC21
										: ELBPublicSNRouteTableAssociation2에는ELBEC22, ELBEC23
	
 



			3. 스택 세부 정보 지정 
				: 스택 이름은 "elblab"
				: KeyName에서 앞서 생성했던 키페어 파일 선택


			4. 스택 옵션 구성 페이지 >> 걍 "다음" 버튼
			5. 검토 >> 걍 "전송" 눌러 최종 생성

			6. 생성된 스택 확인
				: 상태가 CREATED_COMPLETED 로 바뀜 잘 생성된 것. (5분 까지 걸릴 수 있다)
				: 생성된 스택의 리소스 탭으로 들어가면 각 리소스 대해 상세 정보 확인 가능 
					: EC2 인스턴스에 할당된 public IP 확인 위해 이동해봐라





	2. 기본 인프라 환경 검증
		: 절차
			(1) SERVER-1 과 SERVER-2 에 SSH 접속해서 확인해보기
				1. SERVER-1 인스턴스의 상세 페이지로 이동하여 public IP 확인 >> 스택의 리소스탭-ELBEC21의 물리적 ID 클릭해 상세 정보 페이지 이동 : 13.124.73.4

				2. 해당 public IP 로 SSH 접속하여  /var/www/html 의 하위 구조 파악
					: dev 디렉터리가 생성되어있고, xff.php 파일이 있음 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에 mkdir /var/www/html/dev 해놔서 그런거임
						/var/www/html
						├── dev
						│   └── index.html
						├── index.html
						└── xff.php

				2. SERVER-2 인스턴스의 상세 페이지로 이동하여 public IP 확인 >> 스택의 리소스탭-ELBEC21의 물리적 ID 클릭해 상세 정보 페이지 이동 : 13.125.152.224
					 : 참고로 SERVER-3 는 52.78.143.54 , MyEC2 는 3.36.113.66

				3. 해당 public IP 로 SSH 접속하여  tree /var/www/html 의 하위 구조 파악
					: mgt 디렉터리가 생성되어있고, xff.php 파일이 있음 << 이는 앞선 템플릿에서 SERVER-2 인스턴스의 정의부에 mkdir /var/www/html/mgt 해놔서 그런거임
						/var/www/html
						├── index.html
						├── mgt
						│   └── index.html
						└── xff.php


			(1) MyEC2 에 SSH 접속해서 확인해보기
				1. curl 요청하기 편하게 세 서버의 IP주소를 일시적 변수로 저장하기 >> "변수명=IP주소" 꼴
					: .bashrc 에 저장안하고 이렇게 저장한 변수들은 이번 셸에서만 일시적으로 사용 가능
					: 다음을 입력
						EC21=13.124.73.4
						EC22=13.125.152.224
						EC23=52.78.143.54
 
				2. 변수 잘 저장됬나 확인 >> "echo $변수명" 꼴
					: 앞서 해당 변수에 할당했던 값이 셸에 출력됨
					: 다음을 입력
						echo $EC21
						echo $EC22
						echo $EC23


				3. SERVER-1 웹 서비스 확인 <-- SERVER-2 , SERVER-3 에 대해서도 이처럼 진행함 됨
					(1) 기본 웹페이지 확인 >> curl $EC21
						: <h1>ELB LAB Web Server-1</h1> 이 출력된다 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에  echo "<h1>ELB LAB Web Server-1</h1>" > /var/www/html/index.html 을 써놨기 때문이다.

					(2) /dev 경로의 웹 페이지(index.html) 확인 >> curl $EC21/dev/
						: <h1>ELB LAB Dev Web Page</h1>이 출력된다 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에  echo "<h1>ELB LAB Dev Web Page</h1>" > /var/www/html/dev/index.html 을 써놨기 때문이다


					(3) /mgt 경로의 웹 페이지(index.html) 확인 >>  curl $EC21/mgt/
					    : 직접 입력한적이 없는 
						<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
						<html><head>
						<title>404 Not Found</title>
						</head><body>
						<h1>Not Found</h1>
						<p>The requested URL was not found on this server.</p>
						</body></html>
					    이 출력된다. 
							<< 이는 앞선 템플릿에서 SERVER-1 의 인스턴스 정의부에서 딱히 mgt 라는 디렉터리를 생성하지 않았기 때문. ( mgt 리는 디렉터리는 SERVER-2 에 만들어놨다 )




					(4) php 파일 확인 >> curl $EC21/xff.php
					    :  SERVER-1 쪽의 php 파일이 그대로 전달되는게 아니라 (php 특성상 서버사이드 스크립트이므로), 실행된 내용을 response 받는다. 
					       : 그러니까 html 과 같은 정적 파일을 요청했을 때와는 양상이 좀 다른걸 확인 가능

						CloudNeta ELB Test Page

						Sun, 28 Jul 24 11:00:28 +0900

						Current CPU Load:0%

						Last Client IP: 3.36.113.66
						Server Public IP = 13.124.73.4
						Server Private IP: 10.40.1.10



					(4) SNMP 서비스 확인 >> snmpget -v2c -c public $EC21 [ OID값 ]
					     : 실습에서 보안이 강화된 v3 가 아닌 v2c 를 사용한건 보안은 v3 가 더 좋을 지라도 아무래도 암호화같은 로직이 추가되있어, 평문인 v2c 가 걍 실습해보긴 쉬워서 그런 듯. 
					     : -c 는 CommunityString 을 명시하는 옵션명이고, SERVER-1 에서 CommunityString 에 대한 별도 설정을 하지 않았기 때문에 디폴트 CommunityString 값인 public 을 입력함 된다.
					     : 다음의 OID 를 조회
						(1) 1.3.6.1.2.1.1.1.0  >> sysDescr: 장비 설명. 장비 제조사에 따라 크기에 차이가 있음
							: SERVER-1 반환결과 >> SNMPv2-MIB::sysDescr.0 = STRING: Linux SERVER1 4.14.348-265.565.amzn2.x86_64 #1 SMP Fri Jun 28 23:44:17 UTC 2024 x86_64
								: .0 은 ( manager 가 관리하는 장비 중 어떤 장비인거냐 이런걸 나타내는게 아니라)  해당 관리 당하는 장비에 대한 MIB 테이블의 row 를 의미. .0 밖에 안나왔단 소리는 해당 항목에 대한 정보가 1개만 있었다는 것.
						

							:  SERVER-2 반환결과 >> SNMPv2-MIB::sysDescr.0 = STRING: Linux SERVER2 4.14.348-265.565.amzn2.x86_64 #1 SMP Fri Jun 28 23:44:17 UTC 2024 x86_64



						(2) 1.3.6.1.2.1.1.2.0 >> sysObjectID:  장비의 고유한 ID 값. 이 값으로 벤더, 장비 종류 등을 파악 및 관리 가능

							: SERVER-1 반환결과 >> SNMPv2-MIB::sysObjectID.0 = OID: NET-SNMP-MIB::netSnmpAgentOIDs.10


						(3) 1.3.6.1.2.1.1.3.0 >> sysUpTime: 장비가 부팅되어 현재까지 동작한 milli-second 값

							: SERVER-1 반환결과 >> DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (6377902) 17:42:59.02



						(4) 1.3.6.1.2.1.1.5.0 >> sysName: 사용자가 장비에 설정한 장비 이름으로, 설정하지 않으면 Null 값(해당 장비 이름은 IP 주소 혹은 Alias )
							: SERVER-1 반환결과 >> SNMPv2-MIB::sysName.0 = STRING: SERVER1
							: SERVER-2 반환결과 >> SNMPv2-MIB::sysName.0 = STRING: SERVER2
	

	3. ALB를 생성하고 동작 과정을 확인
		: 교차 영역 로드 밸런싱>>
			 : 로드 밸런서의 도메인 네임으로 접속 및 요청 시, 대상그룹으로 묶여있는 인스턴스끼리 n 빵으로 트래픽 분산시킨다
			: dig 명령어를 하든(=단순 IP를 요청하든) curl 명령어를 하든(=어떤 특정 리소스를 요청하든) 인스턴스를 기준으로 n빵 해서 트래픽 분산시킨다

		: 대상 그룹 
			: 대상그룹 생성시 필요한 정보
			    1. 그룹 세부 정보 지정
				(1) 생성될 대상그룹의 이름
					: 실습에서는 ALB-TG 로 함
				(2) 대상그룹이 소속될 VPC
					: 실습에서는 ELB-VPC 로 함

			    2. 대상 등록 >> 그룹 세보 정보 지정 이후, 해당 대상에 포함 시킬 대상(인스턴스 등)을 선정
				: 필수 단계는 아니고 옵션이고 , 걍 체크 박스 클릭함 됨

		: 로드 밸런서 
			: 로드 밸런싱은 기본적으로 "라운드 로빈" 방식으로 동작한다
			: 하나의 로드 밸런서는 두 개 이상의 가용영역을 관리해야하고, 각 가용영역 당 "로드 밸런서 노드" 가 생성된다.
				: 그러니까  하나의 로드 밸런서는 두 개 이상의 "로드 밸런서 노드"로 구성된다.

				: 로드 밸런서 노드 >> 해당 가용영역에서의 로드 밸런서 설정을 기억 및 동작을 담당하는 부분. 
					: 논리적으로는 로드 밸런서 노드가 존재한다는 것을 인지 가능하지만, 개발자 입장에서는 직접적으로 컨트롤 못한다. (ENI 처럼 직접 상세정보 보거나 삭제하거나 그런 작업이 불가) 
					: 로드 밸런서 생성 시 선택한 가용영역 당 하나씩 생성된다
					: 로드 밸런서 노드 당 하나씩 ENI 가 자동으로 생성된다
						: 해당 ENI 가 로드 밸런서 노드가 해당 가용영역의 특정 서브넷에 있는 것과 같이 동작하게 해주고 + public IP 가 할당해준다
					: 라우팅 테이블 등과는 다르게, 생성 당시부터 로드 밸런서 "노드"와 연결될 서브넷을 지정한다.


			: 하나의 로드 밸런서는 하나 이상의 리스너를 가진다
				: ( 로드 밸런서처럼 )별도로 리스너에 대한 탭이 따로 있진 않고, 로드 밸런서를 생성하는 과정에서 생성하거나 이미 생성된 로드 밸런서에 추가하는 방식으로, 로드 밸런서에 의존적이다(로드 밸런서와 별개로는 존재하지 못한다)
				: 아래와 같은 계층(?) 구조
					▶ 특정 로드 밸런서 
						▼프로토콜1:포트1 (=리스너1)
							▶ 조건1과 그에따른 작업(= 리스너1의 규칙1)
							▶ 조건2와 그에따른 작업(= 리스너1의 규칙2)
							▶ 조건3과 그에따른 작업(= 리스너1의 규칙3)
							.
							.
							.
						▼프로토콜1:포트2 (=리스너2)
						▼프로토콜1:포트3 (=리스너3)
						.
						.
						. 
						▼프로토콜2:포트1  (=리스너k)
						▼프로토콜3:포트1  (=리스너n)				

				: 리스너 >> 특정 "프로토콜" && "포트" 를 만족하는 트래픽을 받아들이고,  그에 대해 (해당 리스너에) 정의된 "규칙"들"을 적용한다.
					: 로드 밸런서 생성 시점에도 생성 가능하고, 생성된 이후에도 추가 가능하다
					: 생성 시 필요한 정보
						(1) 프로토콜과 포트 >> 해당 프로토콜 && 포트를 만족하는 트래픽을 listen 하겠다. 받겠다. 
							: 리스너끼리 프로토콜은 같고 포트만 다르게 할 수도 있다
								ex) HTTP:80 이랑 HTTP:81 공존 가능

						(2) 기본 작업 정의 >> 해당 리스너에 정의된 모든 규칙들을 만족하지 않을 경우 수행할 작업을 정의


				: 리스너 규칙 >> 특정 리스너에 소속되어, 해당 request 가 특정 "조건"을 만족할 경우 해당 request 대해 어떤 "작업"을 할 것인지( 보통은 해당 request 를 어디로 라우팅할건지) 정의한다

					: 생성 시 필요한 정보
						(1) 이름 및 태그 >> 새로 추가할 "리스너 규칙"의 이름 
						(2) 조건 추가 >> 해당 조건을 만족하는 request 일 경우만  (3) 에 정의한 작업을 실행
							sol 1. 경로 >> 특정 path 를 만족하는 경우에만

 						(3) 규칙 작업 정의 >> 앞서 정의한 조건을 만족할 경우 어떠한 작업을 실행할 것인지 정의. 
							sol 1. 대상 그룹으로 전달 >> 해당 requet를 해당 대상 그룹으로 라우팅시킴 (가장 일반적)
							sol 2. URL 로 리디렉션
							sol 3. 고정 응답 반환







			: 로드 밸런서 생성시 필요한 정보
				1. 로드 밸런서의 유형 지정 : 아래 3가지 중 1개 택
					(1) ALB : HTTP/HTTPS 요청이 주된 경우, 어플리케이션 아키텍쳐를 대상으로 하는 경우 택
						: 디폴트로 교차영역 로드 밸런싱 활성화 

					(2) NLB : 고정 IP 주소가 필요한 경우, 대규모/초고속이 필요한 경우 택 
					(3) GLB : 서드파티로 로드밸런싱시키고 싶은 경우 택

				2. 기본 구성
					(1) 이름 >> 생성될 로드 밸런서의 이름
						: 실습에서 ALB 는 걍 "ALB" 로 지음

				3. 네트워크 매핑 >> 해당 로드 밸런서가 속하게 될 VPC 뿐 아니라, 분산의 대상이 되는 가용영역과, 그 가용영역 내의 "서브넷"까지 특정한다. 

					(1) VPC >>로드 밸런서가 속하게 될 VPC. 
						: 해당 로드 밸런서가 부하분산 시킬 대상 그룹과 같은 VPC 에 속하게 해야된다
						: 실습에서 ALB는 "ELB-VPC" 택

					(2) 매핑 >> 로드 밸런서 당 필수적으로 2개 이상의 가용영역을 선택하고, 각 가용영역 당 딱 하나의 서브넷을 택해야된다.
						: 실습에서 ALB는 걍 밑에 2개씩 뜨는 거 걍 클릭. ELB-VPC 내의 2개 가용영역과 그에 대한 서브넷 택 


				4. 보안 그룹 >> 로드 밸런서를 통한 라우팅을 할 때 어떤 보안 그룹을 적용할 지 택
					: 실습에서 ALB 는 ,디폴트 보안그룹을 "제거"하고, elblab-ELBSG-어쩌구  선택
						: 템플릿을 통해 생성된 보안 그룹 선택한거임
					

				5. 리스너 및 라우팅 >> 리스너 생성
					: 생성시 필요한 정보는 위에 "리스너"에 대해 정리한 것 참고


			: 생성한 로드 밸런서에서 확인 가능한 정보
				1. 세부 정보탭 >>	
					(1) 로드 밸런서 유형
					(2) 상태 
						: 활성화 비활성화
					(3) 해당 로드 밸런서가 소속된 VPC
					(4) 해당 로드 밸런서와 연결된 가용영역 및 서브넷 
					(5) 로드밸런서의 DNS 이름

				2. 리스너 및 규칙 >> 정의한 리스너 목록
					: 해당 리스너 클릭시 해당 리스너에 대해 정의된 규칙들 확인 및 수정 가능


			: 로드 밸런서의 동작 원리 
				1. "로드 밸런서 노드" 당(=가용영역 당) 하나씩 ENI 가 생성되고, "로드밸런서"의 도메인 네임으로 해당 ENI의 public IP 모두에 접근 가능
					: "서브넷 내 에 인스턴스가 있는 것과 같은 효과"를 내기 위해 "해당 서브넷 내의 ENI"를 "해당 인스턴스"에 부착하는 것처럼, "로드 밸런서 노드가 해당 서브넷 내에 있는 것과 같은 효과"를 내기 위해 해당 서브넷 내의 ENI"를" 로드 밸런서 노드"에 부착하는거다.
					: ENI 하나 당 Public IP 가 할당되고, 

				2. HTTP 클라이언트 입장인 MyEC2 에서 서버 입장인 SERVER-1 , SERVER-2, SERVER-3 으로 접근 시  각 인스턴스로 직접 HTTP 접근을 하는게 아니라 로드 밸런서 의 도메인 주소로 접근하게 됨
				3. 로드 밸런서 생성 당시 정의했던 리스너 정책에 따라 로드 밸런싱이 됨
 
		: 절차
		     (1) 대상그룹 생성
			1. EC2 서비스로 이동- 왼쪽탭의 로드밸런싱 코너에서 대상그룹 메뉴 선택- "대상그룹생성"  버튼 클릭
			2. 위에 정의한 것과 같이 "그룹 세부 정보 지정" 설정 및 다음 버튼 클릭
			3. "대상 등록" 코너에서 뜨는 인스턴스 3개 모두 체크 후 "include as pending below(=아래에 보류 중인 것으로 포함)" 버튼을 클릭
				: 선택한 인스턴스가 생성될 대상에 포함시켜지게 된다

			4. "대상그룹 생성" 버튼 클릭
			5. 생성된 대상그룹 정보 확인

		    (2) ALB 로드 밸런서 생성
			1. 여전히 EC2 서비스에서- 왼쪽탭의 로드밸런싱 코너에서 로드밸런서 메뉴 선택- "로드밸런서생성"  버튼 클릭
			2. 유형으로 ALB 선택
			3. 다음과 같이 설정 및 생성
				(1) 기본 구성
					1. 이름: ALB
				(2) 네트워크 매핑
					1. VPC : "ELB-VPC" 택
					2. 매핑 : 2개 뜨는 가용영역과 그에 대한 서브넷

				(3) 보안 그룹 : 디폴트 보안그룹을 "제거"하고, 템플릿으로 생성된 elblab-ELBSG-어쩌구  선택
 				(4) 리스너 및 라우팅 
					1. 프로토콜과 포트번호: 초기값 HTTP:80 그대로 냅둠
 					2. 대상그룹 : "ALB-TG" 선택

			4. 생성된 로드 밸런서 정보 확인
				: 좀 지나면 프로비저닝 상태에서 활성화 상태로 변경된다.


		    (3) ALB 로드 밸런서의 동작 확인해보기
			1. MyEC2 인스턴스에 SSH 접속
			2. (반복문으로) 90번 로드 밸런서의 도메인네임을 호스트로 하여 index.html 요청해보기(기본 경로면 index.html 요청되는거 알겠징)  >> 각 인스턴스 별로 거의 고르게 요청이 분산됨.
				for i in {1..90}; do curl $ALB --silent ; done | sort | uniq -c | sort -nr

				30 <h1>ELB LAB Web Server-3</h1>
				30 <h1>ELB LAB Web Server-2</h1>
				30 <h1>ELB LAB Web Server-1</h1>


			3. 로드 밸런서의 도메인네임을 호스트로 하여 각 가용영역의 고유한 리소스 요청해보기 >> 어떤 때는 정상 response되고,  어떤 떄는 404 not found 뜸. 즉 특정 path 를 명시해도 교차영역로드밸런싱 기능이 잘 작동하긴하는데 불편함.
				SERVER-1 에만 있는 자원 요청해보기 >> curl $ALB/dev/index.html --silent
					: 트래픽이 SERVER-1 으로 갔을 떄만 정상적인 리소스 반환되고 그 외에는 404 not found 

				SERVER-2, 3 에만 있는 자원 요청해보기 >> curl $ALB/mgt/index.html --silent
					: 트래픽이 SERVER-2,3 으로 갔을 떄만 정상적인 리소스 반환되고 그 외에는 404 not found 
	
				


	4. ALB의 경로 기반 라우팅 기능을 이용한 로드 밸런싱 방법을 구성하고 확인
		: 절차
		    (1) ALB 로드 밸런서에 path 기반 라우팅 적용하기
				1. path 별로 대상 그룹 생성하기 >> 같은 path 를 허용할 인스턴스들로 구성된 대상그룹을 생성한다	
					: 실습에서는 SERVER-1 혼자 dev 그룹, SERVER-2 와 SERVER-3 2개를 mgt 그룹으로 생성
					(1) EC2 서비스 - 로드 밸런싱 탭 - 대상그룹 에서 "대상 그룹 생성" 버튼 클릭
					(2) 그룹 세부 정보 지정 페이지에서 다음과 같이 설정
						1. 대상그룹 이름 : 첫번째로 생성하는건 DEV-TG, 두번째로 생성하는건 MGT-TG
						2. 프로토콜 및 포트 : 둘 다 그대로 HTTP 80 그대로
						3. VPC : 둘다 ELB-VPC

					(3) 대상 등록 페이지에서 해당 대상 그룹에 포함시킬 인스턴스 체크 및 "아래 보류중인 것을 포함" 버튼 클릭 
						: DEV-TG 의 경우 SERVER-1 하나 택
						: MGT-TG 의 경우 SERVER-2, SERVER-3 두 개 택

					(4) "대상 그룹 생성" 버튼 클릭


				2. 리스너 규칙 추가하기 >> (path 별로 생성한 대상그룹별로) path 규칙 적용하기 
					(1) EC2 서비스 - 로드밸런싱탭 - 로드 밸런서 에서 앞서 생성한 로드 밸런서인 "ALB" 를 선택하여 상세 정보 나오게 한 후 "리스너 및 규칙" 탭 클릭 
					(2) 기존에 생성했던 HTTP:80 대한 규칙덩어리(?) 체크하고, 위의 규칙관리 드롭다운 내려서 "규칙 추가" 클릭 
						: 아님 걍 해당 HTTP:80 규칙 덩어리의 규칙 column 의 규칙값 같은거 클릭해서 리스너 규칙 코너로 가고, 거기서 "규칙 추가" 버튼 눌러도 됨.

					(3) 각 대상별로 다음과 같이 설정
					    : DEV-TG 에 대한 리스너 규칙 추가
						1. 이름 및 태그 : dev
						2. 조건 추가 
							(1) 조건 유형 : 경로
							(2) 경로 : /dev/*
						3. 규칙 작업 정의
							(1) 작업 유형 : 대상 그룹으로 전달
							(2) (전달할 대상 그룹으로) DEV-TG 선택

					    : MGT-TG 에 대한 리스너 규칙 추가
						1. 이름 및 태그 : mgt
						2. 조건 추가 
							(1) 조건 유형 : 경로
							(2) 경로 : /mgt/*
						3. 규칙 작업 정의
							(1) 작업 유형 : 대상 그룹으로 전달
							(2) (전달할 대상 그룹으로) DEV-TG 선택


		    (2) 접속해서 잘 적용됬나 확인해보기
			1. MyEC2 인스턴스에 SSH 접속
			2. 로드 밸런서의 도메인네임을 호스트로 하여 각 가용영역의 고유한 리소스 요청해보기 >> 연속으로 정상 response됨
				SERVER-1 에만 있는 자원 요청해보기 >> curl $ALB/dev/index.html --silent
					: 리스너 따라 트래픽이 SERVER-1 로만 감

				SERVER-2, 3 에만 있는 자원 요청해보기 >> curl $ALB/mgt/index.html --silent
					: 리스너 따라 트래픽이 SERVER-2,3로만 감

	5. ALB의 User-Agent 를 활용한 로드 밸런싱(HTTP 헤더 기반의 라우팅)
		: HTTP 프로토콜 >> HTTP 헤더에 다양한 정보 담아 전달
			: User-Agent 필드 >> 요청을 보내는 클라이언트 프로그램의 정보
				: 이 정보를 이용해 서버가 웹 브라우저에 맞는 최적 데이터를 보내거나, 특정 장치의 접근 차단이 가능

		: 절차
			(1) MyEC2 인스턴스에 SSH 접속하여 접근할 public IP , 즉 로드 밸런서 노드의 ENI 의 public IP 구하기 >> dig $ALB +short
				: 43.200.26.61 , 52.79.206.246 나왔음
				: 사실 걍 AWS 의 ENI 탭 가서 직접 확인해봐도 되긴 함

			(2) 접근 막기 전 접근 잘되나 확인해보기 >> 아이폰의 웹 브라우저로 43.200.26.61 ||  52.79.206.246 입력
				: 잘 됨

			(3) 


					




	6. NLB를 생성하고 교차 영역 로드 밸런싱 기능 여부를 동작을 거쳐 확인


	7. ALB와 NLB 의 출발지 IP 보존 방식에 대한 동작 과정 확인
	8. 실습에서 생성한 자원 모두 삭제




	
 


4.1 Amazon ELB

OSI 7 계층 관련
	: 계층이 높아질수록 이전 계층을 기본적으로 깔고가면서 +α 시킨다는 느낌
		: ex )  ARP 가 MAC 을 사용하긴 해도, IP 까지 사용해야해서 2계층이 아닌 3계층에 속함

	:  아무리 더 높은 계층이 기본적으로 그 아래의 계층을 활용한다고 해도, 일반적으로는 그 아래 계층에서는 작동 불가 >> 그 아래 계층 이상의 계층에 속해있다는건, 그 아래 계층 이상의 무언갈 필요로 했단 말이니까. 
		: 그러니까 자신이 속해있는 계층에서만 작동 가능

	: HTTP 는 7계층이었다!


레거시 legacy >>  낡은 기술. 
	: 남아있긴 하지만 더이상 선호되진 않는.

로드 밸런싱 load blanancing , 부하분산 >> 서버 - 클라이언트 환경에서, 서버가 클라이언트 요청을 받아 처리하는 과정에서 발생하는 부하(연산작업)에 대해 동일한 목적을 수행하는 다수의 서버에 분산 처리하는 기능	
	: 간단하게 말하면, 트래픽을 (서버로) 효율적으로 배분하는 기능.
	: "부하분산" == "로드 밸런싱" . 같은 뜻.
	: 부하분산을 사용 효과 >> 고가용성, 내결함성이 향상 
		고가용성 High availability >>시스템을 항시 사용 가능하게 하는 기능
		내결함성 Fault tolerance >>시스템 일부에 결함이 있더라도 계속 작동할 수 있게 하는 능력

로드 밸런서 load balancer >> 부하분산을 실행시키는 대상




Amazon ELB Elastic Load Balancing : Amazon EC2 인스턴스에서 운영중인 애플리케이션, 서비스로 유입되는 트래픽을 자동 분산 처리하는 기술
	: 여러 가용영역에서 작동
	: 지원
		: HTTP(S) , SSL 등 다양한 프로토콜
		: SSL 암호화 
		: 사용자가 같은 인스턴스에서 세션을 유지 가능
		: 네트워크 및 어플리케이션 수준의 로드밸런싱

	: 활용 방안 >> 다른 AWS 서비스와 짬뽕하여 유용하게 쓰인다
		: AWS 의 Cloudwatch 와 함께 >> 로그와 메트릭을 모니터링 
		: AWS 의 Autoscaling 과 함께 >> 트래픽이 증가할 때 자동으로 인스턴스를 추가하거나 제거하면서 애플리케이션 가용성을 유지

	: 원리 
		(0) 클라이언트가 ELB 의 DNS 주소로 트래픽을 전달(= request 를 보냄)
		(1)  로드 밸런서에서 클라이언트 request 를 수신받고, 클라이언트와 연결을 유지한 상태에서, request 를 수신하기 위해 리스너를 "등록"

		(2) 로드밸런서가 수신한 클라이언트 request 를 처리할 대상 그룹을 선택.

		(3) 로드밸런서가 ( 앞서 선택한 "대상 그룹" 내에서 ) request 를 처리할 대상을 딱 특정하고, 해당 대상으로 request 를 분산(=전달). 
			: 로드 밸런서는 각 대상의 상태를 모니터링하고 있어. 가용하지 않은 대상에겐 request 를 분산시키지 않음.
		
		(4) 분산된 request 대해 대상이 반환한 response 를 로드밸런서가 클라이언트에게 반환
 



	: 주요 구성 요소
		: 로드 밸런서는 가용영역 단위로, 리스너는 리전 단위로 배포함으로써 ELB가 각 가용영역으로 트래픽을 분산시키는걸 가능하게 한다.

		1. 로드 밸런서 Load Balancer >> 클라이언트와 직접적으로 소통( request 받고 response 전달 )하는 역할 + 내부적으론 클라이언트로부터 받은 request(트래픽)를 (상태가 정상인) 대상 그룹에 정의된 서버로 분배하는 역할
			: 일종의 라우터.
			: 가용영역 단위로, 로드 밸런서 "노드" 로써 배포된다.
			: 종류 >> 트래픽의 프로토콜종류, 서비스 목정, 대상 등 따라 4가지의 로드 밸런서를 제공
				(1) CLB Classic Load Balancer >> 가장 초기에 출시된 로드 밸런서
					: 잘 안쓴다. 레거시.
						: 서버의 주소가 변경되면 로드 밸런스를 새로 생성해야됨
						: 기능적 한계 (포트, 헤더 등을 수정 불가)
					: 4계층, 7계층 ( Application layer ) 로드 밸런서		
					: 7계층 로드 밸런싱 >>	 HTTP/HTTPS 요청의 내용을 기반으로 트래픽을 분배	
							: 프로토콜 >> HTTP/HTTPS 지원

						: 4계층 로드 밸런싱 >> IP 주소와 포트 번호를 기준으로 트래픽을 분배
							: 프로토콜 >> SSL/TLS 지원


				(2) ALB Application Load Balancer >> 웹 어플리케이션(HTTP/HTTPS)에 특화된 로드 밸런서
					: 7 계층 로드 밸런서 >> HTTP/HTTPS 지원. 
					    : HTTP/HTTPS 프로토콜 기반 다양한 라우팅 기능 제공
						1. URL path 기반 라우팅 
						2. host 기반 라우팅
						3. 쿼리 스트링 기반 라우팅 

					: 람다를 대상 그룹 지정 가능

				(3) NLB Network Load Balancer >> 대규모 트래픽처리와 빠른 응답에 특화된 로드 밸런서
					: 4 계층 로드 밸런서 >> TCP, UDP, SSL/TLS 프로토콜 지원
						: 신뢰성,안정성 등을 이유로 (아무리 대규모라해도) UDP 보단 TCP사용
						: 7계층인 ALB에 비해선 저수준 >> 비교적 빠르고 & 대규모 트래픽을 처리하는데 용이. (= 성능이 좋다)

					: (일반적인 로드 밸런서완 달리) 클라이언트 IP 주소를 원래 IP 주소로 보존 가능
						: 일반적인 로드 밸런서 >> 중간에 로드 밸런서가 클라이언트의 트래픽을 받으면, 클라이언트의 IP 주소를 로드 밸런서 자신의 IP 주소로 대체한다. 

						: NLB에서 이 기능은 특별한 설정 없이 기본적으로 제공 << 이는 NLB가 레이어 4(전송 계층)에서 작동하기 때문
						: 사용자 분석을 해야하는 경우 유용

					: (4가지 로드 맬런서 중 유일하게) 고정 IP 사용 가능

					: 주로 게임 서버, VolP 서비스, 미디어 스트리밍 등에서 사용

			


				(4) GWLB GateWay Load Balancer >> 네트워크 트래픽을 (중간에 거치는, 경유용) 다른 네트워크의 대상 그룹(서드파티)으로 부하분산 처리하는 로드 밸런서
					: 주로 보안 강화 목적으로 사용
					: 3계층 , 4계층 로드 밸런서 >> IP 주소기반 라우팅 과 TCP, UDP 프로토콜 지원
					: GWLB, GWLB Endpoint, GWLB Endpoint Service 
						: GWLB 를 사용하기 위해선 GWLB Endpoint, GWLB Endpoint Service 를 추가적으로 사용 필수
			
						: GWLB >> GWLB Endpoint Service와 서드파티, 서드 파티와 GWLBE 간의 통신을 중계하는 역할
							: GWLB 자체는 다른 네트워크 내부에, 서드파티와 함께 있음

						: GWLB Endpoint Service >> 클라이언트와 GWLB 간의 통신을 중계하는 역할.
							: 클라이언트에게 직접적으로 request 를 받고, response 를 하게 된다. 
							: GWLB 와 함께 다른 네트워크 내부에 있음
						
						: GWLBE GWLB Endpoint >> GWLB와 (실제 목적지로 하는) 대상 그룹간의 통신을 중계하는 역할
							: 실제로 서비스를 제공하는 대상 그룹과 함꼐 있음 


					: https://kim-dragon.tistory.com/167
					: https://choiblog.tistory.com/169


		2. 대상 그룹 Target Group >> 로드 밸런서로부터 트래픽을 분배당할 하나 이상의 엔드포인트(예: EC2 인스턴스, Lambda 함수, IP 주소 등)를 정의하는 그룹
			: 일종의 (트래픽을전송받는) 네트워크 장치 그룹.


		3. 리스너 Listener >> 로드 밸런서가 받은 request 대해 적용할 rule 과 그에 대한 action(=그 규칙을 만족하는 request 일 경우 해당 request 를 어디로 라우팅 시킬건지) 을 정의 
			: 일종의 라우팅 테이블
			: 리전 단위로 배포된다.
			: 허용 가능한 포트와 프로토콜 등을 정의하여, 특정 대상 그룹으로 트래픽을 라우팅할 수 있게 해준다
				: 다양한 프로토콜을 지원



	: ELB 를 생성할 때 로드 밸런서와 통신하는 방식
		: 인터넷 경계 로드 밸런서 >> 외부에서 직접 로드 밸런서에 접근하는 방식
		: 내부 로드 밸런서 >> 외부의 접근이 차단된 격리된 네트워크(=내부 네트워크)에서 로드 밸런서를 사용하는 방식

	: ELB 교차 로드 밸런싱 Cross-Zone Load Balancing 
		: 트래픽 분배를 인스턴스 단위로 n빵해준다
		: 활성화 시킬 수도 있고, 비활성화 시킬 수도 있다
			: 교차로드밸런싱기능이 비활성화된 경우, 각 (상태가 정상인) 가용영역 내에 인스턴스가 몇개냐 그런거 전혀 고려 없이 가용영역 단위로 n빵 시키게된다. 
			: 반면 교차로드밸런싱기능이 활성화된 경우, (상태가 정상인) 가용영역 들 내의 인스턴스들 단위로, 고르게 n 빵 시켜준다.
			: 어떤 서비스를 사용하느냐 따라서 교차 로드 밸런싱 기능이 디폴트로 활성화 되있기도 하고 비활성화 되있기도 하다
		: 기본적으로는 "로드 밸런서 수준"에서 설정하고, (필요하다면) 세부적으로는 "대상 그룹 수준"에서 설정한다
			: 로드 밸런서 수준에서의 설정 >> 교차 로드 밸런싱 기능 자체를 활성화 할건지의 여부. 그러니까 걍 가용영역 단위로 n빵할건지, 대상 그룹 내부의 인스턴스 단위로 n빵할건지를 결정.

			: 대상 그룹 수준에서의 설정 >> (해당 대상그룹이 트래픽을 받는다면) 각 인스턴스가 어느 정도의 비율로 받게 될 건지를 설정. 


-----------------------------------------------------------------------------------------------------

4.2 ALB 와 NLB 를 이용한 로드 밸런싱 구성

프로비저닝 provisioning : (필요할 때 바로 사용가능하도록) 준비해두는 것.
	: https://jake-seo-dev.tistory.com/210


IaC Infrastructure As Code : (수동으로 자원을 만들지 않고) 코드를 통해 자원/인프라를 제공.생성


DevOps


배포 과정 :
	1. 개발자가 코드를 원격 저장소에 올림
	2. 원격 저장소에 올라간 코드가, 아래의 과정을 모두 통과
		(1) build
		(2) test
		(3) release

	3. (모두 통과하고) 빌드된 형태로 배포 서버에 전달됨
	4. 배포 서버가 애플리케이션 서버에 최종 배포를 함

배포 자동화 : 한번의 클릭 혹은 명령어 입력을 통해 전체 배포 과정을 자동으로 진행하는 것


파이프라인 Pipeline : 소스 코드의 관리부터 실제 서비스로의 배포 과정을 연결한 구조.
	: 파이프라인은 전체 배포 과정을 여러 Stage 로 분리
		1. Source 단계 : 원격저장소에 관리되고 있는 소스 코드에 변경 사항이 일어날 경우 이를 감지하고 다음 단계(Build 단계)로 전달.
		2. Build 단계 :  코드를 컴파일/빌드/테스트 및 빌드된 형태로 다음단계(Deploy단계)로 전달.
		3. Deploy 단계 : 빌드를 실제 서비스에 반영

	: ( CI/CD ) 파이프라인을 구축한다 == 배포 자동화 시스템을 구축한다
		CI/CD 
			: CI Continous Integration >> 지속적 통합.
				: 코드의 변경 사항이 자동으로 빌드 및 테스트 되어 애플리케이션에 반영된다
			: CD Continouse Delivery/Deployment >> 지속적 제공/배포
				: (변경사항이 있어 새로 빌드 .. 등 하고 배포될 준비가 되면) 자동으로 배포된다
	: https://velog.io/@edith_0318/CICD-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8


TCP/UDP + 특정번호 == 특정 프로토콜
	TCP 22 : SSH
	TCP 80 : HTTP
	UDP 161 : SNMP


TCP 와 UDP 느낌 >> 일단 둘 다 네트워크 상으로 데이터를 전송하는 경우 쓰이는 프로토콜. 
	: TCP >> 보안. 신뢰성이 좀 필요한 경우
	: UDP >> 걍 요청-응답만 구현하면 됬지, 그다지 신뢰성이 중요하진 않은 경우


HTTP HyperText Transfer Protocol >> 인터넷서 데이터 주고 받을 수 있게하는 표준 프로토콜
	: 웹 서버 - 클라이언트 구조 , request 보내고  response 반환
	: 다양한 메서드 사용 >> GET , POST, PUT , DELTE
	: 80 번 포트 사용
	
Simple Network Management Protocol >>네트워크를 통해 각 네트워크장비들을 모니터링하고 관리할 수 있게하는 프로토콜 
	: SNMP 자체는 "프로토콜"일 뿐, 이를 활용하여 실제 각 네트워크 장비의 정보를 얻기 위해선 이를 이용하고자하는 장비쪽에 별도의 프로그램을 설치 필요
		:  agent 측에는 SNMP Daemon 과 같은, 해당 agnet 와는 전혀 별개의 소프트웨어가 돌아가고 그 소프트웨어가 SNMP 프로토콜을 사용한 통신을 가능하게 하는 거다.

	: SNMP 구성 요소 
		: agent >> 관리 대상. 정보를 제공하는 하드웨어/소프트웨어 
			: 161 번 UDP 포트 사용 , 예외적으로 TRAP 타입 메세지를 전송하는 경우 162번 UDP 포트 사용

		: manager >> 관리시스템/관리자. 정보를 수집하는 하드웨어/소프트웨어 
			: 162번 UDP 포트 사용
			: 현업에서는 manager 을 NMS Network Management System 이라고도 부른다.
	
 	
	: 동작 방식 >> SNMP 관리자와 agent 간에 메세지를 주고 받음
	: ( agent 와 manage 간 ) 통신을 위해선 UDP를 사용, SNMP 자체는 7 계층 프로토콜

	: 버전 >> SNMPv1 , SNMPv2c, SNMPv3 ... 등 계속 업뎃됬음
	     : 주로 Community String 에 대한 보안 강화를 위한 업데이트
		SNMPv1: SNMP의 첫 버전. 보안이 약해서 잘 안 씀 ( 모든 데이터는 평문 )
		SNMPv2/SNMPv2c: v1에서 보안 문제를 좀 개선
		SNMPv3: 최신버전. Username 과 password 를 넣고, 더 강력한 암호화 적용.
		
	: SNMP Message 의 타입
		: Http 메서드 느낌
		(1) SNMP Get : (관리자 쪽에서 보내는) 장비 정보 조회 메시지

		(2) SNMP Set  : (관리자 쪽에서 보내는) 장비 정보 수정 메시지
			: 잘 사용되진 않음. (네트워크 장비 설정은 주로 네트워크 엔지니어가 직접 변경)

		(3) SNMP TRAP : "에이전트 쪽에서 보내는" 메시지
			: 관리자 쪽에서 요청하지 않아도, 이상 상황 발생시 장비가 보내는 메시지 타입. 관리자가 놓칠 수 있는 부분을 보완하는 역할

 

	: 관련 용어
		: Community String 커뮤니티 값 >> agent 와 manger 간의 인증을 위한 값
			: manager 와 agnet 가 서로 동일한 community string 값을 가지고  있어야 통신이 성립하게됨
			: Community String 속성의 종류
				(1) read-only >>에이전트가 매니저로부터 get request 만 받을 수 있다는 것. (그러니까 단순 조회에 대한 응답만 가능)
				(2) read/write >> 에이전트가 매니저로부터 get request 뿐 아니라 set request 도 받을 수 있다는 것 (조회 뿐 아니라 수정에 대한 작업 및 응답이 가능) 

			: 별다른 설정 없을 시 디폴트론 read-only 속성의 "public"이란 문자열이 Community String 으로 지정된다.
			: https://m.blog.naver.com/aepkoreanet/222101737343

		: Object >> 데이터덩어리. agent 와 manager 가 주고 받은 SNMP message 정보
		: OID Object ID >> Object 의 고유 식별자. 
			: SNMP 에서는 각 네트워크 장비의 기능, 설정이 OID 를 기준으로 구별됨
			: 표기 >> 숫자가 '.'을 기준으로 구분되어 나열된 꼴
				: 사실은 OID 는 트리 구조이고, 이를 '.' 을 기준으로 나열해 표기한 것. 
					: OID 앞쪽 일수록 트리 상단부 노드 , 뒤로 갈수록 말단부 노드
					: 각 노드는 특정한 의미를 가진다 (ex: 특정 벤더, 특정 장비 기능)
				: 그러니까 OID 는 의미 없는, 단순 구별하기 위한 식별자가 아니라, 의미가 있는 일관된 식별자이고 (OID를 통해 벤더와 , 해당 장비의 기능 등을 파악 가능), 그렇다고 이걸 외우는 사람은 ㅂㅅ 인거고 걍 검색해라 . 그때그때.
			: 종류
				(1) Public OID >> 벤더 공통적으로 사용되는 OID 
					: System, Interface, IP 등 벤더 상관없이 필수적인 내용을 제공하는 OID

				(2) Private OID >> 벤더별로 지정되어 사용되는 OID 

		: MIB Management Information Base >>  Object 들의 집합. db의 테이블이라고 봄 됨.
			: 그러니까 쉽게 비유하면
				MIB = 테이블
				Object = 테이블의 row
				OID = 테이블 row 의 ID 컬럼값
 
	: https://aws-hyoh.tistory.com/179



기본적인 DNS 의 원리
	: https://yaelimeee.tistory.com/46

CPU 라운드 로빈 Round Robin >> 프로세스 사이에 우선순위를 두지 않고, 순서대로 CPU를 할당하는 방식의  CPU 스케줄링 기법

DNS Round Robin >> DNS 서버 구성 방식 중 하나로, 도메인네임에 대해 여러 서버(의 IP)가 존재할 때, 순서대로, 돌아가면서, 트래픽을 각 서버에게 할당하는 방식
	: https://yaelimeee.tistory.com/46
	


ENI Elastic Network Interface >> 탄력적 네트워크 인터페이스.
	: 일종의 가상 LAN 카드
		: 랜카드 >> 컴퓨터를 네트워크에 연결하는 역할을 하는 부품.

	: 포함 정보. 생성시 설정하게되는 정보
		0. subnet >> 해당 ENI 가 생성될 서브넷
		1. IP 
			(1) private IP
			(2) public IP <-- optional
				: 리소스에 public IP 를 할당할 경우 가지게 된다.
		2. MAC address
		3. 보안 그룹

	: 인스턴스와 ENI
		: 얘가 실질적으로 인스턴스와 인터넷,서브넷, 보안 그룹을 연결시킨다 
			: 사실 인스턴스는 해당 서브넷 내부에 생성되있는게 아니라 그런 것처럼 ENI 가 연결하는거임 + 인스턴스 직접적으로 public IP를 가지는게 아닌데 그런 것처럼 ENI가 제공해주는거임

		: EC2 인스턴스는 반드시 하나 이상의 ENI 와 연결되어있어야된다 
		: ENI 는 인스턴스와 별도로 생성 가능하다
			: 미리 ENI 만들어놨다가 특정 인스턴스 생성할 때 해당 ENI 부착했다가, 다시 그 ENI 떼서 다른 인스턴스에 부착시킬 수 있다. 

	:  로드밸런서와 ENI
		: 로드 밸런서 "노드" 별 로도(= 가용영역 당) ENI 가 자동 생성되고, 각 ENI 에 public IP 가 할당되며 특정 서브넷과 연결된다.
	: https://jibinary.tistory.com/133	
	: https://kimjingo.tistory.com/197


셸 변수
	: 프롬프트 창에 단순히 "변수명=값" 을 입력하여, "해당 셀 세션에서" 일시적으로 사용할 변수를 등록 가능하다
		: 세션이 종료되면 메모리에서 해당 변수는 제거된다. ( SSH 접속이 끊길 때마다도 이전에 설정했던 변수를 사용 불가)	
			: 그렇다고 history 상으로 입력한 명령어 내역에서 까지 사라지는건 아니고, 다만 변수로써의 유효력이 없어지는 것.
		: 셸 변수의 영구적인 저장을 원하면(=새로운 셸 세션이 시작될 때마다 변수가 자동 설정되게 하려면) .bashrc 와 같은 초기화 스크립트에  "export 변수명=값" 꼴로 추가해야한다. 
			: 그리고 이를 설정 후 바로 반영시키려면 source ~/.bashrc

	: 등록한 변수를 사용하는 법 >> $변수명
		: 해당 변수뒤에 바로 다른 문자열 써도 잘 인식된다
			ex ) EC21=13.125.22.69 한 상황에서  $EC21/dev/ 은 13.125.22.69/dev/




리눅스 툴 관련
	curl >> Client URL . URL 을 사용해서 서버에 데이터를 request 하는,클라이언트 역할을 하는 툴.
		: 형식 >> curl [옵션] [URL]
		: 자주 쓰이는 옵션
			-s, --silent : 부가 정보 없이 조회
				: ex) curl -s www.daum.net

			-X : 요청시 사용할 HTTP 메서드의 종류(GET POT PUT PATCH DELTE)를 뒤에 명시 
				: ex ) curl -X GET  http://localhost:8080/user/100

			-d : HTTP POST 요청 데이터 입력

			-H : 전송할 헤더를 지정
				: POST 의 기본 Content type 이 JSON 이 아니라서, JSON 파일을 첨부해보내고 싶으면 별도로 헤더 추가 필수
				: ex ) curl -d '{"key1":"value1", "key2":"value2"}' -H "Content-Type: application/json" -X POST  http://localhost:8080/user/100



		: https://velog.io/@odh0112/Linux-Curl-%EB%AA%85%EB%A0%B9%EC%96%B4


	dig Domain Information Groper >> DNS 에게 질의할 수 있는 툴
		: 그러니까 DNS 에게 해당 도메인의 실제 주소가 뭔지 request 하고, 그에대한 response 를 받을 수 있게 해주는 클라이언트 역할을 해주는 툴이란 것
		: 형식 >> dig [@특정네임서버] [도메인네임] [+옵션명] 
			: +옵션명
				(1) +short : 알잘딱깔센. 질의한 결과(해당 DNS 대한 실제 IP값)만 표시함. 
				(2) +trace 
			: 보통 "dig 도메인네임 +short " 의 꼴로 가장 많이 활용한다. 
		: https://carpfish.tistory.com/entry/DNS-dig-%EB%AA%85%EB%A0%B9%EC%96%B4-%EC%86%8C%EA%B0%9C-%EB%B0%8F-%EC%82%AC%EC%9A%A9%EB%B2%95



html과 자바스크립트와 PHP
	: html >> 요청하면 걍 준다. 정적 웹 페이지
	: 자바스크립트, php >> 별도로 실행하고 전달된다. 동적  웹페이지
		: 자바스크립트 >> 클라이언트 사이드 스크립트. 클라이언트에게 전달된 후에 실행되고 결과 확인
		: php >> 서버 사이드 스크립트. 서버 쪽에서 실행되고 클라이언트에게 전달됨.
			: 민감한 정보를 포함해야하는 경우 유용

CloudFormation >> AWS 가 제공하는 IaC 서비스
	: 즉, 실습 환경을 "코드" 기반으로 AWS 인프라 리소스( VPC, EC2 등 )를 자동으로 생성하는 기술
	: 지속적 배포를 원하는 CI/CD 파이프 라인과 통합하여 인프라 관리 자동화도 가능
	: 주요 구성 요소
	    : 템플릿을 해석하여 스택을 생성하고, 정의된 AWS 인프라를 생성/변경/삭제
		1. 템플릿 >> AWS 인프라를 정의하는 JSON 또는 YAML 형식의 파일 
			: 일종의 설정 파일.
			: 인프라의 속성, 관계, 종속성 등을 정의 가능 <-- 템플릿 덕분에 종속성 관리 ㅈㄴ 편하다고 한다
			: 문법
				1. 파라미터 : 템플릿의 윗부분에 정의된 변수로, 템플릿의 재사용성을 높임
					!Ref 을 사용해 파일 내부에서 해당 값 사용 가능	

			:  프로비저닝을 할 수 있다?


		2. 스택 >> CloudFormation 의 관리 단위로, 리소스들의 묶음
			: 스택 삭제 시 해당 스택에 속한 모든 인프라도 함께 삭제되는 거임

		3. 리소스 >> CloudFormation으로 생성한, 말 그대로 리소스. (EC2 인스턴스 , Amazon S3 버킷 등..) 
			

		4. 이벤트 >>  CloudFormation 의 "스택"에서 발생하는 모든 이벤트(생성/변경/삭제 등)
			

	: 작동 방식
		1. 템플릿 작성
		2. 해당 템플릿을 CloudFormation 서비스에 업로드
		3. CloudFormation서비스는 해당 템플릿에 따라 스택을 생성하거나 업데이트 함
			; 스택 모니터링 가능 (생성 또는 업데이트 다른 이벤트. 로그 확인 가능.) 





실습 절차
	1. 기본 인프라를 CloudFormation 으로 배포
		: 절차
			1. 서비스 - CloudFormation 서비스택 - "스택생성" 버튼 클릭
			2. 템플릿 업로드 >> (기존 템플릿 선택 그대로 체크된 채로 냅두고) 템플릿 지정코너에서 Amazon S3 URL 선택 된채로, URL 을 입력 : 교재에서 제공해준 https://cloudneta-aws-book.s3.ap-northeast-2.amazonaws.com/chapter4/elblab.yaml 
					: CloudFormation 템플릿, 즉 yaml 파일을 다운받을 수 있는 URL 임
					: 해당 파일에 정의된 내용
						(1) Parameters 
							1. keyname: >> EC2 인스턴스에 SSH 접근하기 위해 사용하는 기존의 EC2 키 페어 이름
							2. LatestAmiId: >> 어떤 AMI로 EC2 인스턴스 접근할건지 정의

						(2) Resources >> CloudFormation을 통해 생성할 리소스들
							1. VPC 및 기타 네트워크 리소스들
								(1) VPC
									1. ELBVPC
									2. MyVPC

								(2) IGW
									1. ELBIGW
									2. MyIGW
			
								(3) IGW 에 대한 attachment
									: 단순히 IGW 를 생성한다고 끝이 아니라 특정 VPC 에 attach 하는 과정이 필요
									1. ELBIGWAttachment
									2. MyIGWAttachment

								(3) 라우팅 테이블
									: 이게 라우팅 테이블 정의부가 먼저 나왔긴 한데, 이후엔 결국 VPC 내의 서브넷과 연결되어 사용된다

									1. ELBPublicRT
									2. MyPublicRT

								(4) 라우팅 테이블에 IGW 경로 등록
									1. ELBDefaultPublicRoute
									2. MyDefaultPublicRoute

								(5) 서브넷 정의
									1. ELBPublicSN1
									2. ELBPublicSN2
									3. MyPublicSN


								(6) 서브넷과 라우팅 테이블을 연결
									: 그냥 라우팅 테이블을 생성한다고 서브넷에 연결되는게 아님
									: 라우팅테이블은 2개인데 서브넷은 3개라는 점에서 눈치챌 수 있듯이, 같은 VPC 에 속하는 ELBPublicSN1 와 ELBPublicSN2는 같은 라우팅 테이블을 공유한다. (그러니까 서브넷 별 고유한 라우팅 테이블을 가지게 설정한진 않았다)
									1. ELBPublicSNRouteTableAssociation
									2. ELBPublicSNRouteTableAssociation2
									3. MyPublicSNRouteTableAssociation


							2. 보안 그룹 >> EC2 인스턴스에 적용 가능한 보안 그룹을 정의
								(1) MySG >> SSH, ICMP 허용하게 설정됨
								(2) ELBSG >> HTTP, SNMP, SSH, ICMP 허용하게 설정됨

							3. EC2 인스턴스	
							     : Tags 필드를 통해 name 을 설정 가능
								   : KeyName 필드는 키페어 파일명 받는 필드임

							     : UserData 필드를 통해 초기 설정 내용을 명령어로 설정가능- 실습에서는 이를 통해 필요한 툴 및 파일을 미리 설치해놓음
								
								(1) MyEC2 
									: MyVPC에 배포될 t2.micro 인스턴스

								(2) ELBEC21, ELBEC22, ELBEC23
									: ELBVPC 내부 서브넷에 배포될 세 개의 t2.micro 인스턴스
										: ELBPublicSNRouteTableAssociation에는 ELBEC21
										: ELBPublicSNRouteTableAssociation2에는ELBEC22, ELBEC23
	
 



			3. 스택 세부 정보 지정 
				: 스택 이름은 "elblab"
				: KeyName에서 앞서 생성했던 키페어 파일 선택


			4. 스택 옵션 구성 페이지 >> 걍 "다음" 버튼
			5. 검토 >> 걍 "전송" 눌러 최종 생성

			6. 생성된 스택 확인
				: 상태가 CREATED_COMPLETED 로 바뀜 잘 생성된 것. (5분 까지 걸릴 수 있다)
				: 생성된 스택의 리소스 탭으로 들어가면 각 리소스 대해 상세 정보 확인 가능 
					: EC2 인스턴스에 할당된 public IP 확인 위해 이동해봐라





	2. 기본 인프라 환경 검증
		: 절차
			(1) SERVER-1 과 SERVER-2 에 SSH 접속해서 확인해보기
				1. SERVER-1 인스턴스의 상세 페이지로 이동하여 public IP 확인 >> 스택의 리소스탭-ELBEC21의 물리적 ID 클릭해 상세 정보 페이지 이동 : 13.124.73.4

				2. 해당 public IP 로 SSH 접속하여  /var/www/html 의 하위 구조 파악
					: dev 디렉터리가 생성되어있고, xff.php 파일이 있음 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에 mkdir /var/www/html/dev 해놔서 그런거임
						/var/www/html
						├── dev
						│   └── index.html
						├── index.html
						└── xff.php

				2. SERVER-2 인스턴스의 상세 페이지로 이동하여 public IP 확인 >> 스택의 리소스탭-ELBEC21의 물리적 ID 클릭해 상세 정보 페이지 이동 : 13.125.152.224
					 : 참고로 SERVER-3 는 52.78.143.54 , MyEC2 는 3.36.113.66

				3. 해당 public IP 로 SSH 접속하여  tree /var/www/html 의 하위 구조 파악
					: mgt 디렉터리가 생성되어있고, xff.php 파일이 있음 << 이는 앞선 템플릿에서 SERVER-2 인스턴스의 정의부에 mkdir /var/www/html/mgt 해놔서 그런거임
						/var/www/html
						├── index.html
						├── mgt
						│   └── index.html
						└── xff.php


			(1) MyEC2 에 SSH 접속해서 확인해보기
				1. curl 요청하기 편하게 세 서버의 IP주소를 일시적 변수로 저장하기 >> "변수명=IP주소" 꼴
					: .bashrc 에 저장안하고 이렇게 저장한 변수들은 이번 셸에서만 일시적으로 사용 가능
					: 다음을 입력
						EC21=13.124.73.4
						EC22=13.125.152.224
						EC23=52.78.143.54
 
				2. 변수 잘 저장됬나 확인 >> "echo $변수명" 꼴
					: 앞서 해당 변수에 할당했던 값이 셸에 출력됨
					: 다음을 입력
						echo $EC21
						echo $EC22
						echo $EC23


				3. SERVER-1 웹 서비스 확인 <-- SERVER-2 , SERVER-3 에 대해서도 이처럼 진행함 됨
					(1) 기본 웹페이지 확인 >> curl $EC21
						: <h1>ELB LAB Web Server-1</h1> 이 출력된다 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에  echo "<h1>ELB LAB Web Server-1</h1>" > /var/www/html/index.html 을 써놨기 때문이다.

					(2) /dev 경로의 웹 페이지(index.html) 확인 >> curl $EC21/dev/
						: <h1>ELB LAB Dev Web Page</h1>이 출력된다 << 이는 앞선 템플릿에서 SERVER-1 인스턴스의 정의부에  echo "<h1>ELB LAB Dev Web Page</h1>" > /var/www/html/dev/index.html 을 써놨기 때문이다


					(3) /mgt 경로의 웹 페이지(index.html) 확인 >>  curl $EC21/mgt/
					    : 직접 입력한적이 없는 
						<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
						<html><head>
						<title>404 Not Found</title>
						</head><body>
						<h1>Not Found</h1>
						<p>The requested URL was not found on this server.</p>
						</body></html>
					    이 출력된다. 
							<< 이는 앞선 템플릿에서 SERVER-1 의 인스턴스 정의부에서 딱히 mgt 라는 디렉터리를 생성하지 않았기 때문. ( mgt 리는 디렉터리는 SERVER-2 에 만들어놨다 )




					(4) php 파일 확인 >> curl $EC21/xff.php
					    :  SERVER-1 쪽의 php 파일이 그대로 전달되는게 아니라 (php 특성상 서버사이드 스크립트이므로), 실행된 내용을 response 받는다. 
					       : 그러니까 html 과 같은 정적 파일을 요청했을 때와는 양상이 좀 다른걸 확인 가능

						CloudNeta ELB Test Page

						Sun, 28 Jul 24 11:00:28 +0900

						Current CPU Load:0%

						Last Client IP: 3.36.113.66
						Server Public IP = 13.124.73.4
						Server Private IP: 10.40.1.10



					(4) SNMP 서비스 확인 >> snmpget -v2c -c public $EC21 [ OID값 ]
					     : 실습에서 보안이 강화된 v3 가 아닌 v2c 를 사용한건 보안은 v3 가 더 좋을 지라도 아무래도 암호화같은 로직이 추가되있어, 평문인 v2c 가 걍 실습해보긴 쉬워서 그런 듯. 
					     : -c 는 CommunityString 을 명시하는 옵션명이고, SERVER-1 에서 CommunityString 에 대한 별도 설정을 하지 않았기 때문에 디폴트 CommunityString 값인 public 을 입력함 된다.
					     : 다음의 OID 를 조회
						(1) 1.3.6.1.2.1.1.1.0  >> sysDescr: 장비 설명. 장비 제조사에 따라 크기에 차이가 있음
							: SERVER-1 반환결과 >> SNMPv2-MIB::sysDescr.0 = STRING: Linux SERVER1 4.14.348-265.565.amzn2.x86_64 #1 SMP Fri Jun 28 23:44:17 UTC 2024 x86_64
								: .0 은 ( manager 가 관리하는 장비 중 어떤 장비인거냐 이런걸 나타내는게 아니라)  해당 관리 당하는 장비에 대한 MIB 테이블의 row 를 의미. .0 밖에 안나왔단 소리는 해당 항목에 대한 정보가 1개만 있었다는 것.
						

							:  SERVER-2 반환결과 >> SNMPv2-MIB::sysDescr.0 = STRING: Linux SERVER2 4.14.348-265.565.amzn2.x86_64 #1 SMP Fri Jun 28 23:44:17 UTC 2024 x86_64



						(2) 1.3.6.1.2.1.1.2.0 >> sysObjectID:  장비의 고유한 ID 값. 이 값으로 벤더, 장비 종류 등을 파악 및 관리 가능

							: SERVER-1 반환결과 >> SNMPv2-MIB::sysObjectID.0 = OID: NET-SNMP-MIB::netSnmpAgentOIDs.10


						(3) 1.3.6.1.2.1.1.3.0 >> sysUpTime: 장비가 부팅되어 현재까지 동작한 milli-second 값

							: SERVER-1 반환결과 >> DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (6377902) 17:42:59.02



						(4) 1.3.6.1.2.1.1.5.0 >> sysName: 사용자가 장비에 설정한 장비 이름으로, 설정하지 않으면 Null 값(해당 장비 이름은 IP 주소 혹은 Alias )
							: SERVER-1 반환결과 >> SNMPv2-MIB::sysName.0 = STRING: SERVER1
							: SERVER-2 반환결과 >> SNMPv2-MIB::sysName.0 = STRING: SERVER2
	

	3. ALB를 생성하고 동작 과정을 확인
		: 교차 영역 로드 밸런싱>>
			 : 로드 밸런서의 도메인 네임으로 접속 및 요청 시, 대상그룹으로 묶여있는 인스턴스끼리 n 빵으로 트래픽 분산시킨다
			: dig 명령어를 하든(=단순 IP를 요청하든) curl 명령어를 하든(=어떤 특정 리소스를 요청하든) 인스턴스를 기준으로 n빵 해서 트래픽 분산시킨다

		: 대상 그룹 
			: 대상그룹 생성시 필요한 정보
			    1. 그룹 세부 정보 지정
				(1) 생성될 대상그룹의 이름
					: 실습에서는 ALB-TG 로 함
				(2) 대상그룹이 소속될 VPC
					: 실습에서는 ELB-VPC 로 함

			    2. 대상 등록 >> 그룹 세보 정보 지정 이후, 해당 대상에 포함 시킬 대상(인스턴스 등)을 선정
				: 필수 단계는 아니고 옵션이고 , 걍 체크 박스 클릭함 됨

		: 로드 밸런서 
			: 로드 밸런싱은 기본적으로 "라운드 로빈" 방식으로 동작한다
			: 하나의 로드 밸런서는 두 개 이상의 가용영역을 관리해야하고, 각 가용영역 당 "로드 밸런서 노드" 가 생성된다.
				: 그러니까  하나의 로드 밸런서는 두 개 이상의 "로드 밸런서 노드"로 구성된다.

				: 로드 밸런서 노드 >> 해당 가용영역에서의 로드 밸런서 설정을 기억 및 동작을 담당하는 부분. 
					: 논리적으로는 로드 밸런서 노드가 존재한다는 것을 인지 가능하지만, 개발자 입장에서는 직접적으로 컨트롤 못한다. (ENI 처럼 직접 상세정보 보거나 삭제하거나 그런 작업이 불가) 
					: 로드 밸런서 생성 시 선택한 가용영역 당 하나씩 생성된다
					: 로드 밸런서 노드 당 하나씩 ENI 가 자동으로 생성된다
						: 해당 ENI 가 로드 밸런서 노드가 해당 가용영역의 특정 서브넷에 있는 것과 같이 동작하게 해주고 + public IP 가 할당해준다
					: 라우팅 테이블 등과는 다르게, 생성 당시부터 로드 밸런서 "노드"와 연결될 서브넷을 지정한다.


			: 하나의 로드 밸런서는 하나 이상의 리스너를 가진다
				: ( 로드 밸런서처럼 )별도로 리스너에 대한 탭이 따로 있진 않고, 로드 밸런서를 생성하는 과정에서 생성하거나 이미 생성된 로드 밸런서에 추가하는 방식으로, 로드 밸런서에 의존적이다(로드 밸런서와 별개로는 존재하지 못한다)
				: 아래와 같은 계층(?) 구조
					▶ 특정 로드 밸런서 
						▼프로토콜1:포트1 (=리스너1)
							▶ 조건1과 그에따른 작업(= 리스너1의 규칙1)
							▶ 조건2와 그에따른 작업(= 리스너1의 규칙2)
							▶ 조건3과 그에따른 작업(= 리스너1의 규칙3)
							.
							.
							.
						▼프로토콜1:포트2 (=리스너2)
						▼프로토콜1:포트3 (=리스너3)
						.
						.
						. 
						▼프로토콜2:포트1  (=리스너k)
						▼프로토콜3:포트1  (=리스너n)				

				: 리스너 >> 특정 "프로토콜" && "포트" 를 만족하는 트래픽을 받아들이고,  그에 대해 (해당 리스너에) 정의된 "규칙"들"을 적용한다.
					: 로드 밸런서 생성 시점에도 생성 가능하고, 생성된 이후에도 추가 가능하다
					: 생성 시 필요한 정보
						(1) 프로토콜과 포트 >> 해당 프로토콜 && 포트를 만족하는 트래픽을 listen 하겠다. 받겠다. 
							: 리스너끼리 프로토콜은 같고 포트만 다르게 할 수도 있다
								ex) HTTP:80 이랑 HTTP:81 공존 가능

						(2) 기본 작업 정의 >> 해당 리스너에 정의된 모든 규칙들을 만족하지 않을 경우 수행할 작업을 정의


				: 리스너 규칙 >> 특정 리스너에 소속되어, 해당 request 가 특정 "조건"을 만족할 경우 해당 request 대해 어떤 "작업"을 할 것인지( 보통은 해당 request 를 어디로 라우팅할건지) 정의한다

					: 생성 시 필요한 정보
						(1) 이름 및 태그 >> 새로 추가할 "리스너 규칙"의 이름 
						(2) 조건 추가 >> 해당 조건을 만족하는 request 일 경우만  (3) 에 정의한 작업을 실행
							sol 1. 경로 >> 특정 path 를 만족하는 경우에만

 						(3) 규칙 작업 정의 >> 앞서 정의한 조건을 만족할 경우 어떠한 작업을 실행할 것인지 정의. 
							sol 1. 대상 그룹으로 전달 >> 해당 requet를 해당 대상 그룹으로 라우팅시킴 (가장 일반적)
							sol 2. URL 로 리디렉션
							sol 3. 고정 응답 반환







			: 로드 밸런서 생성시 필요한 정보
				1. 로드 밸런서의 유형 지정 : 아래 3가지 중 1개 택
					(1) ALB : HTTP/HTTPS 요청이 주된 경우, 어플리케이션 아키텍쳐를 대상으로 하는 경우 택
						: 디폴트로 교차영역 로드 밸런싱 활성화 

					(2) NLB : 고정 IP 주소가 필요한 경우, 대규모/초고속이 필요한 경우 택 
					(3) GLB : 서드파티로 로드밸런싱시키고 싶은 경우 택

				2. 기본 구성
					(1) 이름 >> 생성될 로드 밸런서의 이름
						: 실습에서 ALB 는 걍 "ALB" 로 지음

				3. 네트워크 매핑 >> 해당 로드 밸런서가 속하게 될 VPC 뿐 아니라, 분산의 대상이 되는 가용영역과, 그 가용영역 내의 "서브넷"까지 특정한다. 

					(1) VPC >>로드 밸런서가 속하게 될 VPC. 
						: 해당 로드 밸런서가 부하분산 시킬 대상 그룹과 같은 VPC 에 속하게 해야된다
						: 실습에서 ALB는 "ELB-VPC" 택

					(2) 매핑 >> 로드 밸런서 당 필수적으로 2개 이상의 가용영역을 선택하고, 각 가용영역 당 딱 하나의 서브넷을 택해야된다.
						: 실습에서 ALB는 걍 밑에 2개씩 뜨는 거 걍 클릭. ELB-VPC 내의 2개 가용영역과 그에 대한 서브넷 택 


				4. 보안 그룹 >> 로드 밸런서를 통한 라우팅을 할 때 어떤 보안 그룹을 적용할 지 택
					: 실습에서 ALB 는 ,디폴트 보안그룹을 "제거"하고, elblab-ELBSG-어쩌구  선택
						: 템플릿을 통해 생성된 보안 그룹 선택한거임
					

				5. 리스너 및 라우팅 >> 리스너 생성
					: 생성시 필요한 정보는 위에 "리스너"에 대해 정리한 것 참고


			: 생성한 로드 밸런서에서 확인 가능한 정보
				1. 세부 정보탭 >>	
					(1) 로드 밸런서 유형
					(2) 상태 
						: 활성화 비활성화
					(3) 해당 로드 밸런서가 소속된 VPC
					(4) 해당 로드 밸런서와 연결된 가용영역 및 서브넷 
					(5) 로드밸런서의 DNS 이름

				2. 리스너 및 규칙 >> 정의한 리스너 목록
					: 해당 리스너 클릭시 해당 리스너에 대해 정의된 규칙들 확인 및 수정 가능


			: 로드 밸런서의 동작 원리 
				1. "로드 밸런서 노드" 당(=가용영역 당) 하나씩 ENI 가 생성되고, "로드밸런서"의 도메인 네임으로 해당 ENI의 public IP 모두에 접근 가능
					: "서브넷 내 에 인스턴스가 있는 것과 같은 효과"를 내기 위해 "해당 서브넷 내의 ENI"를 "해당 인스턴스"에 부착하는 것처럼, "로드 밸런서 노드가 해당 서브넷 내에 있는 것과 같은 효과"를 내기 위해 해당 서브넷 내의 ENI"를" 로드 밸런서 노드"에 부착하는거다.
					: ENI 하나 당 Public IP 가 할당되고, 

				2. HTTP 클라이언트 입장인 MyEC2 에서 서버 입장인 SERVER-1 , SERVER-2, SERVER-3 으로 접근 시  각 인스턴스로 직접 HTTP 접근을 하는게 아니라 로드 밸런서 의 도메인 주소로 접근하게 됨
				3. 로드 밸런서 생성 당시 정의했던 리스너 정책에 따라 로드 밸런싱이 됨
 
		: 절차
		     (1) 대상그룹 생성
			1. EC2 서비스로 이동- 왼쪽탭의 로드밸런싱 코너에서 대상그룹 메뉴 선택- "대상그룹생성"  버튼 클릭
			2. 위에 정의한 것과 같이 "그룹 세부 정보 지정" 설정 및 다음 버튼 클릭
			3. "대상 등록" 코너에서 뜨는 인스턴스 3개 모두 체크 후 "include as pending below(=아래에 보류 중인 것으로 포함)" 버튼을 클릭
				: 선택한 인스턴스가 생성될 대상에 포함시켜지게 된다

			4. "대상그룹 생성" 버튼 클릭
			5. 생성된 대상그룹 정보 확인

		    (2) ALB 로드 밸런서 생성
			1. 여전히 EC2 서비스에서- 왼쪽탭의 로드밸런싱 코너에서 로드밸런서 메뉴 선택- "로드밸런서생성"  버튼 클릭
			2. 유형으로 ALB 선택
			3. 다음과 같이 설정 및 생성
				(1) 기본 구성
					1. 이름: ALB
				(2) 네트워크 매핑
					1. VPC : "ELB-VPC" 택
					2. 매핑 : 2개 뜨는 가용영역과 그에 대한 서브넷

				(3) 보안 그룹 : 디폴트 보안그룹을 "제거"하고, 템플릿으로 생성된 elblab-ELBSG-어쩌구  선택
 				(4) 리스너 및 라우팅 
					1. 프로토콜과 포트번호: 초기값 HTTP:80 그대로 냅둠
 					2. 대상그룹 : "ALB-TG" 선택

			4. 생성된 로드 밸런서 정보 확인
				: 좀 지나면 프로비저닝 상태에서 활성화 상태로 변경된다.


		    (3) ALB 로드 밸런서의 동작 확인해보기
			1. MyEC2 인스턴스에 SSH 접속
			2. (반복문으로) 90번 로드 밸런서의 도메인네임을 호스트로 하여 index.html 요청해보기(기본 경로면 index.html 요청되는거 알겠징)  >> 각 인스턴스 별로 거의 고르게 요청이 분산됨.
				for i in {1..90}; do curl $ALB --silent ; done | sort | uniq -c | sort -nr

				30 <h1>ELB LAB Web Server-3</h1>
				30 <h1>ELB LAB Web Server-2</h1>
				30 <h1>ELB LAB Web Server-1</h1>


			3. 로드 밸런서의 도메인네임을 호스트로 하여 각 가용영역의 고유한 리소스 요청해보기 >> 어떤 때는 정상 response되고,  어떤 떄는 404 not found 뜸. 즉 특정 path 를 명시해도 교차영역로드밸런싱 기능이 잘 작동하긴하는데 불편함.
				SERVER-1 에만 있는 자원 요청해보기 >> curl $ALB/dev/index.html --silent
					: 트래픽이 SERVER-1 으로 갔을 떄만 정상적인 리소스 반환되고 그 외에는 404 not found 

				SERVER-2, 3 에만 있는 자원 요청해보기 >> curl $ALB/mgt/index.html --silent
					: 트래픽이 SERVER-2,3 으로 갔을 떄만 정상적인 리소스 반환되고 그 외에는 404 not found 
	
				


	4. ALB의 경로 기반 라우팅 기능을 이용한 로드 밸런싱 방법을 구성하고 확인
		: 절차
		    (1) ALB 로드 밸런서에 path 기반 라우팅 적용하기
				1. path 별로 대상 그룹 생성하기 >> 같은 path 를 허용할 인스턴스들로 구성된 대상그룹을 생성한다	
					: 실습에서는 SERVER-1 혼자 dev 그룹, SERVER-2 와 SERVER-3 2개를 mgt 그룹으로 생성
					(1) EC2 서비스 - 로드 밸런싱 탭 - 대상그룹 에서 "대상 그룹 생성" 버튼 클릭
					(2) 그룹 세부 정보 지정 페이지에서 다음과 같이 설정
						1. 대상그룹 이름 : 첫번째로 생성하는건 DEV-TG, 두번째로 생성하는건 MGT-TG
						2. 프로토콜 및 포트 : 둘 다 그대로 HTTP 80 그대로
						3. VPC : 둘다 ELB-VPC

					(3) 대상 등록 페이지에서 해당 대상 그룹에 포함시킬 인스턴스 체크 및 "아래 보류중인 것을 포함" 버튼 클릭 
						: DEV-TG 의 경우 SERVER-1 하나 택
						: MGT-TG 의 경우 SERVER-2, SERVER-3 두 개 택

					(4) "대상 그룹 생성" 버튼 클릭


				2. 리스너 규칙 추가하기 >> (path 별로 생성한 대상그룹별로) path 규칙 적용하기 
					(1) EC2 서비스 - 로드밸런싱탭 - 로드 밸런서 에서 앞서 생성한 로드 밸런서인 "ALB" 를 선택하여 상세 정보 나오게 한 후 "리스너 및 규칙" 탭 클릭 
					(2) 기존에 생성했던 HTTP:80 대한 규칙덩어리(?) 체크하고, 위의 규칙관리 드롭다운 내려서 "규칙 추가" 클릭 
						: 아님 걍 해당 HTTP:80 규칙 덩어리의 규칙 column 의 규칙값 같은거 클릭해서 리스너 규칙 코너로 가고, 거기서 "규칙 추가" 버튼 눌러도 됨.

					(3) 각 대상별로 다음과 같이 설정
					    : DEV-TG 에 대한 리스너 규칙 추가
						1. 이름 및 태그 : dev
						2. 조건 추가 
							(1) 조건 유형 : 경로
							(2) 경로 : /dev/*
						3. 규칙 작업 정의
							(1) 작업 유형 : 대상 그룹으로 전달
							(2) (전달할 대상 그룹으로) DEV-TG 선택

					    : MGT-TG 에 대한 리스너 규칙 추가
						1. 이름 및 태그 : mgt
						2. 조건 추가 
							(1) 조건 유형 : 경로
							(2) 경로 : /mgt/*
						3. 규칙 작업 정의
							(1) 작업 유형 : 대상 그룹으로 전달
							(2) (전달할 대상 그룹으로) DEV-TG 선택


		    (2) 접속해서 잘 적용됬나 확인해보기
			1. MyEC2 인스턴스에 SSH 접속
			2. 로드 밸런서의 도메인네임을 호스트로 하여 각 가용영역의 고유한 리소스 요청해보기 >> 연속으로 정상 response됨
				SERVER-1 에만 있는 자원 요청해보기 >> curl $ALB/dev/index.html --silent
					: 리스너 따라 트래픽이 SERVER-1 로만 감

				SERVER-2, 3 에만 있는 자원 요청해보기 >> curl $ALB/mgt/index.html --silent
					: 리스너 따라 트래픽이 SERVER-2,3로만 감

	5. ALB의 User-Agent 를 활용한 로드 밸런싱(HTTP 헤더 기반의 라우팅)
		: HTTP 프로토콜 >> HTTP 헤더에 다양한 정보 담아 전달
			: User-Agent 필드 >> 요청을 보내는 클라이언트 프로그램의 정보
				: 이 정보를 이용해 서버가 웹 브라우저에 맞는 최적 데이터를 보내거나, 특정 장치의 접근 차단이 가능

		: 절차
			(1) MyEC2 인스턴스에 SSH 접속하여 접근할 public IP , 즉 로드 밸런서 노드의 ENI 의 public IP 구하기 >> dig $ALB +short
				: 43.200.26.61 , 52.79.206.246 나왔음
				: 사실 걍 AWS 의 ENI 탭 가서 직접 확인해봐도 되긴 함

			(2) 접근 막기 전 접근 잘되나 확인해보기 >> 아이폰의 웹 브라우저로 43.200.26.61 ||  52.79.206.246 입력
				: 잘 됨

			(3) 


					




	6. NLB를 생성하고 교차 영역 로드 밸런싱 기능 여부를 동작을 거쳐 확인


	7. ALB와 NLB 의 출발지 IP 보존 방식에 대한 동작 과정 확인
	8. 실습에서 생성한 자원 모두 삭제




	
 


